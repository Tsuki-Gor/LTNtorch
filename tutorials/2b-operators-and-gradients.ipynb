{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Complementary Notebook: Appropriate Operators to Approximate Connectives and Quantifiers\n",
    "\n",
    "This notebook is a complement to the tutorial on operators (2-grounding_connectives.ipynb).\n",
    "\n",
    "Logical connectives are grounded in LTN using fuzzy semantics. However, while all fuzzy logic operators make sense when simply *querying* the language, not every operator is equally suited for *learning*.\n",
    "\n",
    "We will see common problems of some fuzzy semantics and which operators are better for the task of *learning*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "当然，以下是您提供内容的中文翻译：\n",
    "\n",
    "## 补充笔记本：用于近似连词和量词的适当算子\n",
    "\n",
    "本笔记本是关于算子的教程（2-grounding_connectives.ipynb）的补充。\n",
    "\n",
    "逻辑连词在LTN中使用模糊语义进行基础化。然而，尽管在简单查询语言时所有模糊逻辑算子都能合理使用，但并不是每个算子都同样适合用于学习。\n",
    "\n",
    "我们将看到一些模糊语义的常见问题，以及哪些算子更适合用于学习任务。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import ltn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:19:50.312036Z",
     "start_time": "2024-08-05T02:19:46.609734Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Querying\n",
    "\n",
    "One can access the implementation of the most common fuzzy semantics in the `ltn.fuzzy_ops` module.\n",
    "They are implemented using PyTorch primitives.\n",
    "\n",
    "Here, we compare:\n",
    "- the product t-norm: $u \\land_{\\mathrm{prod}} v = uv$,\n",
    "- the Lukasiewicz t-norm: $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$,\n",
    "- the minimum aggregator: $\\min(u_1,\\dots,u_n)$,\n",
    "- the p-mean error aggregator (generalized mean of the deviations w.r.t. the truth): $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$.\n",
    "\n",
    "Each operator obviously conveys very different meanings, but they can all make sense depending on the intent of the query.\n",
    "\n",
    "In the following, it is possible to observe that different semantics for the conjunction return very different results.\n",
    "The same behavior can be observed when comparing different aggregators computed on the same input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 查询\n",
    "\n",
    "可以在 `ltn.fuzzy_ops` 模块中访问最常见的模糊语义的实现。它们使用 PyTorch 原语实现。\n",
    "\n",
    "在这里，我们比较：\n",
    "- 乘积 t-范数： $u \\land_{\\mathrm{prod}} v = uv$，\n",
    "- Lukasiewicz t-范数： $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$，\n",
    "- 最小聚合器： $\\min(u_1,\\dots,u_n)$，\n",
    "- p-均值误差聚合器（相对于真值的偏差的广义均值）： $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$。\n",
    "\n",
    "每个运算符显然传达了非常不同的意义，但根据查询的意图，它们都可以有意义。\n",
    "\n",
    "在下面的例子中，可以观察到不同的合取语义返回的结果非常不同。同样的行为也可以在对相同输入计算不同聚合器时观察到。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x1 = torch.tensor(0.4)\n",
    "x2 = torch.tensor(0.7)\n",
    "\n",
    "# the stable keyword is explained at the end of the notebook # stable关键字在本文末尾解释\n",
    "and_prod = ltn.fuzzy_ops.AndProd(stable=False)\n",
    "and_luk = ltn.fuzzy_ops.AndLuk()\n",
    "\n",
    "print(and_prod(x1, x2))\n",
    "print(and_luk(x1, x2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:19:54.460524Z",
     "start_time": "2024-08-05T02:19:54.413544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2800)\n",
      "tensor(0.1000)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1])\n",
    "\n",
    "# the stable keyword is explained at the end of the notebook # stable关键字在本文末尾解释\n",
    "forall_min = ltn.fuzzy_ops.AggregMin() # forall_min 是一个最小聚合器\n",
    "forall_pME = ltn.fuzzy_ops.AggregPMeanError(p=4, stable=False) # forall_pME 是一个 p-均值误差聚合器\n",
    "\n",
    "print(forall_min(xs, dim=0)) # xs应该是一维张量，dim=0 表示在第0维上进行聚合\n",
    "print(forall_pME(xs, dim=0))\n",
    "\n",
    "print(xs.shape)\n",
    "print(xs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:21:05.704870Z",
     "start_time": "2024-08-05T02:21:05.685320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000)\n",
      "tensor(0.3134)\n",
      "torch.Size([8])\n",
      "tensor([1.0000, 1.0000, 1.0000, 0.5000, 0.3000, 0.2000, 0.2000, 0.1000])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While all operators are suitable in a querying setting, this not the case in a learning setting. Indeed, many fuzzy logic operators have derivatives not suitable for gradient-based algorithms. For more details, read [van Krieken et al., *Analyzing Differentiable Fuzzy Logic Operators*, 2020](https://arxiv.org/abs/2002.06100).\n",
    "\n",
    "Here, we give simple illustrations of such gradient issues.\n",
    "\n",
    "#### 1. Vanishing Gradients\n",
    "\n",
    "Some operators have vanishing gradients on some part of their domains.\n",
    "\n",
    "For example, in $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$, if $u+v-1 < 0$, the gradients vanish.\n",
    "\n",
    "In the following, it is possible to observe an edge case in which the Lukasiewicz conjunction leads to vanishing gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "当然，以下是您提供的内容的中文翻译：\n",
    "\n",
    "虽然所有操作符在查询设置中都适用，但在学习设置中情况并非如此。实际上，许多模糊逻辑操作符的导数不适合基于梯度的算法。有关详细信息，请阅读 [van Krieken 等人，*分析可微模糊逻辑操作符*，2020](https://arxiv.org/abs/2002.06100)。\n",
    "\n",
    "在这里，我们给出了一些关于梯度问题的简单说明。\n",
    "\n",
    "#### 1. 梯度消失\n",
    "\n",
    "某些操作符在其定义域的某些部分会出现梯度消失的情况。\n",
    "\n",
    "例如，在 $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$ 中，如果 $u+v-1 < 0$，则梯度消失。\n",
    "\n",
    "在下面的例子中，可以观察到一个极端情况，在这种情况下，Lukasiewicz 合取导致梯度消失。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x1 = torch.tensor(0.3, requires_grad=True) # requires_grad=True 启用梯度计算，这样在进行反向传播时可以计算它们的梯度。\n",
    "x2 = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "y = and_luk(x1, x2)\n",
    "y.backward()  # this is necessary to compute the gradients # 这是必要的，以计算梯度\n",
    "# 调用 y.backward() 进行反向传播，计算 x1 和 x2 的梯度。这一步是必要的，因为我们启用了梯度计算。\n",
    "res = y.item() # 使用 y.item() 获取张量 y 的标量值并赋值给 res。\n",
    "gradients = [v.grad for v in [x1, x2]] # 使用列表推导式获取 x1 和 x2 的梯度，存储在 gradients 列表中。\n",
    "# print the result of the aggregation # 打印聚合结果\n",
    "print(res)\n",
    "# print gradients of x1 and x2 # 打印x1和x2的梯度\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:25:02.463074Z",
     "start_time": "2024-08-05T02:25:02.440866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[tensor(0.), tensor(0.)]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Single-Passing Gradients\n",
    "\n",
    "Some operators have gradients propagating to only one input at a time, meaning that all other inputs will not benefit from learning at this step.\n",
    "\n",
    "An example is the minimum aggregator, namely $\\min(u_1,\\dots,u_n)$.\n",
    "\n",
    "In the following, it is possible to observe an edge case in which the `Min` aggregator leads to singe-passing gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. 单次传递梯度\n",
    "\n",
    "某些运算符的梯度在任意时刻只会传播到一个输入，这意味着所有其他输入在这一步骤中不会从学习中受益。\n",
    "\n",
    "一个例子是最小值聚合器，即 $\\min(u_1,\\dots,u_n)$。\n",
    "\n",
    "在下文中，可以观察到 `Min` 聚合器导致单次传递梯度的一个极端情况。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_min(xs, dim=0)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation # 打印聚合结果\n",
    "print(res)\n",
    "# print gradients of xs # 打印xs的梯度\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:33:04.940687Z",
     "start_time": "2024-08-05T02:33:04.890329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000149011612\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Exploding Gradients\n",
    "\n",
    "Some operators have exploding gradients on some part of their domains.\n",
    "\n",
    "An example is the `PMean` aggregator, namely $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$.\n",
    "\n",
    "In the edge case where all inputs are $1.0$, this operator leads to exploding gradients.\n",
    "\n",
    "In the following, it is possible to observe this behavior."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. 梯度爆炸\n",
    "\n",
    "某些运算符在其某些域上会出现梯度爆炸现象。\n",
    "\n",
    "一个例子是 `PMean` 聚合器，即\n",
    "$$\n",
    "\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "在所有输入均为 $1.0$ 的边缘情况下，该运算符会导致梯度爆炸。\n",
    "\n",
    "在下文中，可以观察到这种行为。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xs = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=4)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation # 打印聚合结果\n",
    "print(res)\n",
    "# print the gradients of xs # 打印xs的梯度\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:40:12.736991Z",
     "start_time": "2024-08-05T02:40:12.718964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "tensor([nan, nan, nan])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stable Product Configuration\n",
    "\n",
    "#### Product Configuration\n",
    "\n",
    "In general, we recommend using the following \"product configuration\" in LTN:\n",
    "* not: the standard negation  $\\lnot u = 1-u$,\n",
    "* and: the product t-norm $u \\land v = uv$,\n",
    "* or: the product t-conorm (probabilistic sum) $u \\lor v = u+v-uv$,\n",
    "* implication: the Reichenbach implication $u \\rightarrow v = 1 - u + uv$,\n",
    "* existential quantification (\"exists\"): the generalized mean (p-mean) $\\mathrm{pM}(u_1,\\dots,u_n) = \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n u_i^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$,\n",
    "* universal quantification (\"for all\"): the generalized mean of \"the deviations w.r.t. the truth\" (p-mean error) $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$.\n",
    "\n",
    "#### \"Stable\"\n",
    "\n",
    "As is, this \"product configuration\" is not fully exempt from issues:\n",
    "- the product t-norm has vanishing gradients on the edge case $u=v=0$;\n",
    "- the product t-conorm has vanishing gradients on the edge case $u=v=1$;\n",
    "- the Reichenbach implication has vanishing gradients on the edge case $u=0$,$v=1$;\n",
    "- `pMean` has exploding gradients on the edge case $u_1=\\dots=u_n=0$;\n",
    "- `pMeanError` has exploding gradients on the edge case $u_1=\\dots=u_n=1$.\n",
    "\n",
    "However, all these issues happen on edge cases and can easily be fixed using the following \"trick\":\n",
    "- if the edge case happens when an input $u$ is $0$, we modify every input with $u' = (1-\\epsilon)u+\\epsilon$;\n",
    "- if the edge case happens when an input $u$ is $1$, we modify every input with $u' = (1-\\epsilon)u$;\n",
    "\n",
    "where $\\epsilon$ is a small positive value (e.g. $1\\mathrm{e}{-5}$).\n",
    "\n",
    "This \"trick\" gives us a stable version of such operators. Stable in the sense it has not gradient issues anymore.\n",
    "\n",
    "One can trigger the stable version of such operators by using the boolean parameter `stable`. It is possible to set a default\n",
    "value for `stable` when initializing the operator, or to use different values at each call of the operator.\n",
    "\n",
    "In the following, we repeat the last example with the difference that we are now using the stable version of the `pMean`\n",
    "operator. It is possible to observe that the gradients are now different from `NaN`. Thanks to the stable verison of the\n",
    "operator, we are now able to obtain suitable gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 稳定的乘积配置\n",
    "\n",
    "#### 乘积配置\n",
    "\n",
    "一般来说，我们建议在 LTN 中使用以下“乘积配置”：\n",
    "* 否定：标准的否定 $\\lnot u = 1-u$，\n",
    "* 与：乘积 t-范数 $u \\land v = uv$，\n",
    "* 或：乘积 t-余范数（概率和） $u \\lor v = u+v-uv$，\n",
    "* 蕴含：Reichenbach 蕴含 $u \\rightarrow v = 1 - u + uv$，\n",
    "* 存在量化（“存在”）：广义平均（p-平均） $\\mathrm{pM}(u_1,\\dots,u_n) = \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n u_i^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$，\n",
    "* 全称量化（“对于所有”）：广义平均误差（p-平均误差） $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$。\n",
    "\n",
    "#### “稳定”\n",
    "\n",
    "按现状，这种“乘积配置”并非完全没有问题：\n",
    "- 乘积 t-范数在边界情况 $u=v=0$ 时具有消失梯度；\n",
    "- 乘积 t-余范数在边界情况 $u=v=1$ 时具有消失梯度；\n",
    "- Reichenbach 蕴含在边界情况 $u=0, v=1$ 时具有消失梯度；\n",
    "- `pMean` 在边界情况 $u_1=\\dots=u_n=0$ 时具有爆炸梯度；\n",
    "- `pMeanError` 在边界情况 $u_1=\\dots=u_n=1$ 时具有爆炸梯度。\n",
    "\n",
    "然而，所有这些问题都发生在边界情况下，并且可以使用以下“技巧”轻松修复：\n",
    "- 如果边界情况发生在输入 $u$ 为 $0$ 时，我们修改每个输入为 $u' = (1-\\epsilon)u+\\epsilon$；\n",
    "- 如果边界情况发生在输入 $u$ 为 $1$ 时，我们修改每个输入为 $u' = (1-\\epsilon)u$；\n",
    "\n",
    "其中 $\\epsilon$ 是一个小的正值（例如 $1\\mathrm{e}{-5}$）。\n",
    "\n",
    "这个“技巧”使得我们能够获得这些算子的稳定版本。稳定的意思是不再有梯度问题。\n",
    "\n",
    "可以通过使用布尔参数 `stable` 触发这些算子的稳定版本。在初始化算子时可以设置 `stable` 的默认值，或者在每次调用算子时使用不同的值。\n",
    "\n",
    "在下面的示例中，我们重复了上一个示例，但现在我们使用了 `pMean` 算子的稳定版本。可以观察到梯度现在不再是 `NaN`。感谢算子的稳定版本，我们现在能够获得合适的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xs = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "\n",
    "# the exploding gradient problem is solved # 爆炸梯度问题得到解决\n",
    "y = forall_pME(xs, dim=0, p=4, stable=True)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation # 打印聚合结果\n",
    "print(res)\n",
    "# print the gradients of xs # 打印xs的梯度\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:41:37.112635Z",
     "start_time": "2024-08-05T02:41:37.100078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998999834060669\n",
      "tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The hyper-parameter $p$ in the generalized means\n",
    "\n",
    "The hyper-parameter $p$ of `pMean` and `pMeanError` offers flexibility in writing more or less strict formulas, to\n",
    "account for outliers in the data depending on the application. However, $p$ should be carefully set since it could have\n",
    "strong implications for the training of LTN.\n",
    "\n",
    "In the following, we see how a huge increase of $p$ leads to single-passing gradients in the `pMean` operator. This is\n",
    "intuitive as in the second tutorial we have observed that `pMean` tends to the `Max` when $p$ tends to infinity. Similar\n",
    "to the `Min` aggregator (seen before in this tutorial), the `Max` aggregator leads to single-passing gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 广义均值中的超参数 $ p $\n",
    "\n",
    "`pMean` 和 `pMeanError` 的超参数 $ p $ 提供了在编写严格程度不同的公式时的灵活性，以便根据应用情况处理数据中的异常值。然而， $ p $ 的设置应谨慎，因为它可能对 LTN 的训练产生重大影响。\n",
    "\n",
    "在下文中，我们将看到 $ p $ 的大幅增加如何导致 `pMean` 操作符中的单次传递梯度。这是直观的，因为在第二个教程中我们已经观察到，当 $ p $ 趋向于无穷大时，`pMean` 趋向于 `Max`。与 `Min` 聚合器（在本教程前面部分看到的）类似，`Max` 聚合器也会导致单次传递梯度。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=4)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print result of aggregation # 打印聚合结果\n",
    "print(res)\n",
    "# print gradients of xs # 打印xs的梯度\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:48:14.338475Z",
     "start_time": "2024-08-05T02:48:14.310599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31339913606643677\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0483, 0.1325, 0.1977, 0.1977, 0.2815])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=20)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print result of aggregation # 打印聚合结果\n",
    "print(res)\n",
    "# print gradients of xs # 打印xs的梯度\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-05T02:49:02.660463Z",
     "start_time": "2024-08-05T02:49:02.652902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18157517910003662\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0734e-05, 6.4147e-03, 8.1100e-02,\n",
      "        8.1100e-02, 7.6019e-01])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "While it can be tempting to set a high value for $p$ when querying, in a learning setting, this can quickly lead to a \"single-passing\" operator that will focus too much on outliers at each step (i.e., gradients overfitting one input at this step, potentially harming the training of the others). We recommend not to set a too high $p$ when learning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "以下是您提供的内容的中文翻译：\n",
    "\n",
    "尽管在查询时将 $p$ 值设置得很高可能很有诱惑力，但在学习环境中，这很快会导致“单次通过”操作符在每一步过多关注异常值（即在这一步中梯度过拟合一个输入，可能会损害其他输入的训练）。我们建议在学习时不要将 $p$ 值设置得过高。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
