{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Grounding in Logic Tensor Networks (LTN)\n",
    "\n",
    "## Real Logic\n",
    "\n",
    "The semantics of LTN differs from the standard abstract semantics of First-order Logic (FOL). Specifically, LTN domains are interpreted concretely by tensors in the Real field.\n",
    "\n",
    "To emphasize the fact that LTN interprets symbols which are grounded on real-valued features, we use the term *grounding*, denoted by $\\mathcal{G}$, instead of interpretation.\n",
    "$\\mathcal{G}$ associates a tensor of real numbers to any term of the language, and a real number in the interval $[0,1]$ to any formula $\\phi$.\n",
    "\n",
    "In the rest of the tutorials, we commonly use \"tensor\" to designate \"tensor in the Real field\".\n",
    "\n",
    "Real Logic language consists of a non-logical part (the signature) and logical connectives and quantifiers.\n",
    "* **constants** denote individuals from some space of tensors $\\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$ (tensor of any rank). The individuals can be pre-defined (data point) or learnable (embedding).\n",
    "* **variables** denote sequences of individuals.\n",
    "* **functions** can be any mathematical function either pre-defined or learnable. Examples of functions can be distance functions, regressors, etc. Functions can be defined using any operations in PyTorch. They can be linear functions, Deep Neural Networks, and so forth.\n",
    "* **predicates** are represented as mathematical functions that map from some n-ary domain of individuals to a real number in $[0,1]$, which can be interpreted as a truth degree. Examples of predicates can be similarity measures, classifiers, etc.\n",
    "* **connectives** ($\\lnot, \\land, \\lor, \\implies$) are modeled using fuzzy connective operators.\n",
    "* **quantifiers** are defined using fuzzy aggregators.\n",
    "\n",
    "This tutorial explains how to ground constants, variables, functions, and predicates.\n",
    "\n",
    "In order to properly use LTN, we need to import the framework and PyTorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 逻辑张量网络（LTN）中的基础\n",
    "\n",
    "#### 现实逻辑\n",
    "\n",
    "LTN的语义与一阶逻辑（FOL）的标准抽象语义不同。具体而言，LTN域在实数域中通过张量具体解释。\n",
    "\n",
    "为了强调LTN解释基于实值特征的符号，我们使用术语“基础”（grounding），表示由 $\\mathcal{G}$ 代替解释。 $\\mathcal{G}$ 将实数张量与语言中的任何术语关联，并将区间 $[0, 1]$ 中的实数与任何公式 $\\phi$ 关联。\n",
    "\n",
    "在其余的教程中，我们通常使用“张量”来表示“实数域中的张量”。\n",
    "\n",
    "现实逻辑语言由非逻辑部分（签名）和逻辑连接词及量词组成。\n",
    "- **常量** 表示某些张量空间中的个体 $\\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$ （任何秩的张量）。这些个体可以是预定义的（数据点）或可学习的（嵌入）。\n",
    "- **变量** 表示个体的序列。\n",
    "- **函数** 可以是任何预定义或可学习的数学函数。函数的例子可以是距离函数、回归器等。函数可以使用PyTorch中的任何操作定义。它们可以是线性函数、深度神经网络等。\n",
    "- **谓词** 表示将某些n元域的个体映射到区间 $[0, 1]$ 中的数学函数，该区间可以解释为一个真值度。谓词的例子可以是相似性度量、分类器等。\n",
    "- **连接词** 使用模糊连接操作符建模（例如，¬, ∧, ∨, ⇒,还有等价）。在fuzzy_ops.py中定义了这些操作符。\n",
    "- **量词** 使用模糊聚合器定义。就是量化，有存在量词和全称量词两种。具体模糊聚合器，可以在fuzzy_ops.py中找到。\n",
    "\n",
    "本教程解释了如何将常量、变量、函数和谓词基础化。\n",
    "\n",
    "为了正确使用LTN，我们需要导入框架和PyTorch。\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import ltn\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:15:50.939610Z",
     "start_time": "2024-08-03T08:15:46.947595Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants\n",
    "\n",
    "LTN constants are grounded into real tensors. Each constant $c$ is mapped to a point in $\\mathcal{G}(c) \\in \\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$. Notice that the objects in the domain may be tensors of any rank. For instance, a tensor of rank $0$ corresponds to a scalar, a tensor of rank $1$ to a vector, a tensor of rank $2$ to a matrix and so forth, in the usual way.\n",
    "\n",
    "Here, we define $\\mathcal{G}(c_1)=(2.1,3)$ and $\\mathcal{G}(c_2)=\\begin{pmatrix}\n",
    "4.2 & 3 & 2.5\\\\\n",
    "4 & -1.3 & 1.8\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "Note that the `Constant` constructor needs a PyTorch tensor containing the features of the individual."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 常量\n",
    "\n",
    "LTN常量被映射为实数张量。每个常量 \\( c \\) 被映射到一个点 $\\mathcal{G}(c) \\in \\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$。注意，域中的对象可以是任意阶的张量。例如，阶数为0的张量对应于标量，阶数为1的张量对应于向量，阶数为2的张量对应于矩阵，以此类推，这与通常的方式相同。\n",
    "\n",
    "在这里，我们定义 $\\mathcal{G}(c_1)=(2.1,3)$ 和 $\\mathcal{G}(c_2)=\\begin{pmatrix}\n",
    "4.2 & 3 & 2.5\\\\\n",
    "4 & -1.3 & 1.8\n",
    "\\end{pmatrix}$。\n",
    "\n",
    "注意，`Constant` 构造函数需要一个包含个体特征的 PyTorch 张量。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2024/07/2024_07_28__21_51_41_78746c.png)"
  },
  {
   "cell_type": "code",
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([[4.2, 3, 2.5], [4, -1.3, 1.8]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:15:58.438119Z",
     "start_time": "2024-08-03T08:15:58.272871Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that a constant can be set as learnable by using the optional argument `trainable=True`. This is useful to learn embeddings for some individuals.\n",
    "The features of the tensor will be considered as trainable parameters (learning in LTN is explained in a further notebook)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "请注意，可以通过使用可选参数 `trainable=True` 将常量设置为可学习的。这对于为某些个体学习嵌入非常有用。张量的特征将被视为可训练参数（在后续的笔记本中会解释LTN中的学习）。"
  },
  {
   "cell_type": "code",
   "source": [
    "c3 = ltn.Constant(torch.tensor([0.,0.]), trainable=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:15:59.706964Z",
     "start_time": "2024-08-03T08:15:59.701512Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under the hood, LTN constants are implemented using `LTNObject` objects. These particular objects wrap a value (the individual) and an important attribute called `free_vars`, which will be explained later in the tutorial.\n",
    "\n",
    "If the parameter `trainable` of the LTN constant is set to `True`, then\n",
    "the `requires_grad` parameter of the PyTorch tensor contained in the LTN constant will be set to `True`.\n",
    "\n",
    "The value of an LTN constant can be easily accessed using the `value` attribute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在底层实现中，LTN常量使用 `LTNObject` 对象。这些特定对象封装了一个值（个体）和一个重要属性 `free_vars`，后者将在稍后的教程中进行解释。\n",
    "如果 LTN 常量的参数 `trainable` 被设置为 `True`，那么 LTN 常量中包含的 PyTorch 张量的 `requires_grad` 参数将被设置为 `True`。（这个requires_grad应该是在core中的例子中涉及过）\n",
    "可以通过 `value` 属性轻松访问 LTN 常量的值。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(c1.value)\n",
    "print(c3.value)\n",
    "# here, we have to perform a detach before calling numpy(), because the tensor has requires_grad=True # 在调用numpy()之前，我们必须执行一个detach，因为张量的requires_grad=True\n",
    "print(type(c3.value.detach()))\n",
    "# print(c3.value.detach().numpy()) # 原始，会报错\n",
    "# print(c3.value.cpu().detach().numpy()) # c3.value.cpu()：将 c3.value 张量从 CUDA 设备移动到 CPU。\n",
    "print(c3.value.detach().cpu().numpy()) # 这样的话，原始的c3.value这个原始的张量应该还在GPU上。（不对，好像也会去到CPU上）\n",
    "\n",
    "# detach方法该方法返回一个新的张量，从当前计算图中分离出来。因此，这个新的张量将不需要梯度计算。\n",
    "'''\n",
    "c3.value.cpu().detach().numpy()和c3.value.detach().cpu().numpy()都是可以的，只是detach()和cpu()的顺序不同。\n",
    "\n",
    "在 `c3.value.cpu().detach().numpy()` 和 `c3.value.detach().cpu().numpy()` 之间，有顺序上的区别，但在这种情况下，效果是相同的。让我们详细解释一下这两个表达式。\n",
    "\n",
    "### 表达式 1: `c3.value.cpu().detach().numpy()`\n",
    "```python\n",
    "c3.value.cpu().detach().numpy()\n",
    "```\n",
    "1. **`c3.value.cpu()`**：\n",
    "   - 将 `c3.value` 张量从 CUDA 设备移动到 CPU。\n",
    "\n",
    "2. **`.detach()`**：\n",
    "   - 创建一个新的张量，与 `c3.value` 共享数据，但不会跟踪计算图梯度（即，不需要计算梯度）。\n",
    "\n",
    "3. **`.numpy()`**：\n",
    "   - 将张量转换为 NumPy 数组。\n",
    "\n",
    "### 表达式 2: `c3.value.detach().cpu().numpy()`\n",
    "```python\n",
    "c3.value.detach().cpu().numpy()\n",
    "```\n",
    "1. **`c3.value.detach()`**：\n",
    "   - 创建一个新的张量，与 `c3.value` 共享数据，但不会跟踪计算图梯度（即，不需要计算梯度）。\n",
    "\n",
    "2. **`.cpu()`**：\n",
    "   - 将新的张量从 CUDA 设备移动到 CPU。\n",
    "\n",
    "3. **`.numpy()`**：\n",
    "   - 将张量转换为 NumPy 数组。\n",
    "\n",
    "### 区别\n",
    "\n",
    "虽然这两个表达式的结果是相同的，但在执行顺序上有微妙的差异：\n",
    "\n",
    "- **`c3.value.cpu().detach().numpy()`**：首先将张量移动到 CPU，然后分离张量，最后转换为 NumPy 数组。\n",
    "- **`c3.value.detach().cpu().numpy()`**：首先分离张量，然后将分离的张量移动到 CPU，最后转换为 NumPy 数组。\n",
    "\n",
    "### 细节上的区别\n",
    "\n",
    "- **执行顺序**：顺序的不同在某些极端情况下（如张量在 CUDA 上的操作依赖）可能会影响结果，但在大多数情况下，这两种顺序的操作结果是相同的。\n",
    "- **性能**：从性能角度来看，两个表达式的执行效率几乎相同，因为张量在移动到 CPU 之前或之后都需要进行相同的操作。\n",
    "\n",
    "这两种方法都将产生相同的 NumPy 数组，因为最终的张量都被移动到 CPU 并进行了分离，然后转换为 NumPy 数组。\n",
    "\n",
    "因此，选择哪种方式更多的是个人偏好和代码风格的问题。\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:23:05.918028Z",
     "start_time": "2024-08-03T08:23:05.903666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1000, 3.0000], device='cuda:0')\n",
      "tensor([0., 0.], device='cuda:0', requires_grad=True)\n",
      "<class 'torch.Tensor'>\n",
      "[0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nc3.value.cpu().detach().numpy()和c3.value.detach().cpu().numpy()都是可以的，只是detach()和cpu()的顺序不同。\\n\\n在 `c3.value.cpu().detach().numpy()` 和 `c3.value.detach().cpu().numpy()` 之间，有顺序上的区别，但在这种情况下，效果是相同的。让我们详细解释一下这两个表达式。\\n\\n### 表达式 1: `c3.value.cpu().detach().numpy()`\\n```python\\nc3.value.cpu().detach().numpy()\\n```\\n1. **`c3.value.cpu()`**：\\n   - 将 `c3.value` 张量从 CUDA 设备移动到 CPU。\\n\\n2. **`.detach()`**：\\n   - 创建一个新的张量，与 `c3.value` 共享数据，但不会跟踪计算图梯度（即，不需要计算梯度）。\\n\\n3. **`.numpy()`**：\\n   - 将张量转换为 NumPy 数组。\\n\\n### 表达式 2: `c3.value.detach().cpu().numpy()`\\n```python\\nc3.value.detach().cpu().numpy()\\n```\\n1. **`c3.value.detach()`**：\\n   - 创建一个新的张量，与 `c3.value` 共享数据，但不会跟踪计算图梯度（即，不需要计算梯度）。\\n\\n2. **`.cpu()`**：\\n   - 将新的张量从 CUDA 设备移动到 CPU。\\n\\n3. **`.numpy()`**：\\n   - 将张量转换为 NumPy 数组。\\n\\n### 区别\\n\\n虽然这两个表达式的结果是相同的，但在执行顺序上有微妙的差异：\\n\\n- **`c3.value.cpu().detach().numpy()`**：首先将张量移动到 CPU，然后分离张量，最后转换为 NumPy 数组。\\n- **`c3.value.detach().cpu().numpy()`**：首先分离张量，然后将分离的张量移动到 CPU，最后转换为 NumPy 数组。\\n\\n### 细节上的区别\\n\\n- **执行顺序**：顺序的不同在某些极端情况下（如张量在 CUDA 上的操作依赖）可能会影响结果，但在大多数情况下，这两种顺序的操作结果是相同的。\\n- **性能**：从性能角度来看，两个表达式的执行效率几乎相同，因为张量在移动到 CPU 之前或之后都需要进行相同的操作。\\n\\n这两种方法都将产生相同的 NumPy 数组，因为最终的张量都被移动到 CPU 并进行了分离，然后转换为 NumPy 数组。\\n\\n因此，选择哪种方式更多的是个人偏好和代码风格的问题。\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ".numpy() 方法会将张量转换为NumPy数组，但是这会立即将数据从GPU（如果有的话）转移到CPU上，并且会脱离PyTorch的计算图，从而失去梯度信息。因此，对于需要梯度的张量，直接调用 .numpy() 是不安全的，因为这样会中断梯度流的传递。\n",
    "\n",
    "可能的解决方案是使用 .detach() 方法来创建一个不需要梯度的新的张量副本，然后在这个副本上调用 .numpy()。.你按照提示修改成tensor.detach().numpy()试试"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicates\n",
    "\n",
    "LTN predicates are functions which map from some n-ary space of input values to a value in [0., 1.]. Predicates in LTN can be neural networks or any other function that achieves such a mapping.\n",
    "\n",
    "There are different ways to construct a predicate in LTN. The constructor `ltn.Predicate(model, func)` permits two types of construction:\n",
    "- if the `model` parameter is not `None`, the predicate is constructed by using the `torch.nn.Module` model instance that has been given as input;\n",
    "- if the `model` parameter is `None` and the `func` parameter is not `None`, the predicate is constructed by using the function given in input. The construction by using\n",
    "a function is suggested for small mathematical operations with **no weight tracking** (non-trainable functions).\n",
    "\n",
    "The following defines a predicate $P_1$ using the `func` parameter and a predicate $P_2$ using the `model` parameter.\n",
    "\n",
    "Notice the access to the `value` attribute for the constant `mu` inside the predicate $P_1$ definition. We need to do that because\n",
    "`x` is a PyTorch tensor, while `mu` an LTN constant. The operand - is not supported for these two different types. In general, every time\n",
    "an LTN object (constant or variable) is used inside the definition of a predicate, we need to access its value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 谓词\n",
    "\n",
    "LTN 谓词是将某些 n 元空间的输入值映射到 [0., 1.] 范围内的函数。LTN 中的谓词可以是神经网络或任何其他实现这种映射的函数。\n",
    "\n",
    "在 LTN 中有不同的方法来构造谓词。构造函数 `ltn.Predicate(model, func)` 允许两种构造方式：\n",
    "- 如果 `model` 参数不是 `None`，则谓词通过使用作为输入给定的 `torch.nn.Module` 模型实例来构造；\n",
    "- 如果 `model` 参数为 `None` 且 `func` 参数不是 `None`，则谓词通过使用输入的函数来构造。建议使用函数进行构造，用于不需要权重跟踪的小型数学运算（不可训练的函数）。\n",
    "\n",
    "以下定义了使用 `func` 参数的谓词 $P_1$ 和使用 `model` 参数的谓词 $P_2$。\n",
    "\n",
    "注意在谓词 $P_1$ 的定义中，常量 `mu` 的 `value` 属性的访问。我们需要这样做是因为 `x` 是一个 PyTorch 张量，而 `mu` 是一个 LTN 常量。对于这两种不同类型的操作数，- 运算符是不支持的。一般来说，每当在谓词定义中使用 LTN 对象（常量或变量）时，我们需要访问其值。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "mu = ltn.Constant(torch.tensor([2., 3.]))\n",
    "P1 = ltn.Predicate(func=lambda x: torch.exp(-torch.norm(x - mu.value, dim=1)))\n",
    "\n",
    "class ModelP2(torch.nn.Module):\n",
    "    \"\"\"For more info on how to use torch.nn.Module:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModelP2, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dense1 = torch.nn.Linear(2, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "# modelP2 = ModelP2() # 原来这个写法的话，modelP2这个模型好像是在CPU上的，导致后来print(P2(c3).value)报错\n",
    "modelP2 = ModelP2().to(ltn.device)\n",
    "P2 = ltn.Predicate(model=modelP2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:26:24.535526Z",
     "start_time": "2024-08-03T08:26:24.526984Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily query predicates using LTN constants and LTN variables (see further in this notebook to query using variables).\n",
    "\n",
    "Notice the access to the `value` attribute on the evaluation of the predicates.（注意对谓词求值时要访问`value`属性。） We need to do that because LTN predicates\n",
    "return `LTNObject` instances. Inside the `value` attribute of the returned `LTNObject` there is the actual value of the predicate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(ltn.device)\n",
    "\n",
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]))\n",
    "c3 = ltn.Constant(torch.tensor([3.0, 4.8]))\n",
    "# c1 = ltn.Constant(torch.tensor([2.1, 3]).to(ltn.device))\n",
    "# c2 = ltn.Constant(torch.tensor([4.5, 0.8]).to(ltn.device))\n",
    "# c3 = ltn.Constant(torch.tensor([3.0, 4.8]).to(ltn.device))\n",
    "print(P1(c1).value)\n",
    "print(P1(c2).value)\n",
    "print(P2(c3).value)\n",
    "value1=P2(c3).value\n",
    "print(P1(c1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:43:54.919437Z",
     "start_time": "2024-08-03T08:43:54.892153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor(0.9048, device='cuda:0')\n",
      "tensor(0.0358, device='cuda:0')\n",
      "tensor(0.3186, device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "LTNObject(value=tensor(0.9048, device='cuda:0'), free_vars=[])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTES:\n",
    "- If an LTN predicate (or an LTN function) takes several inputs, e.g. $P_4(x_1,x_2)$, the arguments must be separated by a comma.\n",
    "- LTN internally converts the inputs such that there is a \"batch\" dimension on the first dim. Therefore, most operations should work with `dim=1` (dimension of the features).\n",
    "\n",
    "Here, we show the implementation of a binary predicate in LTN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "注意事项：\n",
    "- 如果一个 LTN 谓词（或 LTN 函数）有多个输入，例如 \\(P_4(x_1, x_2)\\)，那么参数必须用逗号分隔。\n",
    "- LTN 内部将输入转换，使得在第一维度上有一个“批处理”维度。因此，大多数操作应该使用 `dim=1`（特征的维度）。\n",
    "\n",
    "在这里，我们展示了 LTN 中二元谓词的实现。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class ModelP4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelP4, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dense1 = torch.nn.Linear(4, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "# P4 = ltn.Predicate(ModelP4()) # 原来的写法会导致报错\n",
    "P4 = ltn.Predicate(ModelP4()).to(ltn.device)\n",
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]))\n",
    "print(P4(c1, c2).value) # multiple arguments are passed as a list # 多个参数作为列表传递"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:47:27.199848Z",
     "start_time": "2024-08-03T08:47:27.185200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6524, device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more details and useful ways to create predicates (incl. how to integrate multiclass classifiers as binary predicates) see the examples included in the /examples folder of this repository."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "\n",
    "LTN functions are grounded as any mathematical function that maps $n$ individuals to one individual in the tensor domain.\n",
    "\n",
    "There are different ways to construct a function in LTN. The constructor `ltn.Function(model, func)` permits two types of construction:\n",
    "- if the `model` parameter is not `None`, the function is constructed by using the `torch.nn.Module` model instance that has been given as input;\n",
    "- if the `model` parameter is `None` and the `func` parameter is not `None`, the function is constructed by using the function given in input. The construction by using\n",
    "a function is suggested for small mathematical operations with **no weight tracking** (non-trainable function).\n",
    "\n",
    "The following defines a function $f_1$ using the `func` parameter, and a function $f_2$ using the `model` parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 函数\n",
    "\n",
    "LTN函数被作为任何将$n$个个体映射到张量域中一个个体的数学函数来表示。\n",
    "\n",
    "在LTN中构造函数有不同的方法。构造函数 `ltn.Function(model, func)` 允许两种类型的构造：\n",
    "- 如果 `model` 参数不是 `None`，则通过使用作为输入给出的 `torch.nn.Module` 模型实例来构造函数；\n",
    "- 如果 `model` 参数为 `None` 且 `func` 参数不是 `None`，则通过使用输入的函数来构造函数。使用函数进行构造建议用于**不进行权重跟踪**的小型数学操作（不可训练的函数）。\n",
    "\n",
    "下面定义了一个使用 `func` 参数的函数 $f_1$，以及一个使用 `model` 参数的函数 $f_2$。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "f1 = ltn.Function(func=lambda x, y: x - y)\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(2, 10)\n",
    "        self.dense2 = torch.nn.Linear(10, 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        return self.dense2(x)\n",
    "\n",
    "model_f2 = MyModel().to(ltn.device)\n",
    "f2 = ltn.Function(model=model_f2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:52:52.049120Z",
     "start_time": "2024-08-03T08:52:52.042746Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily compute functions using LTN constants and LTN variables as inputs (see further in this notebook to query using variables).\n",
    "\n",
    "Notice that the output of an LTN function is an `LTNObject`, as we have already seen for the predicates.\n",
    "\n",
    "The consideration about the necessity of accessing the `value` parameter of the returned `LTNObject` remains the same also for the LTN functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]))\n",
    "print(f1(c1, c2).value) # multiple arguments are passed as a list\n",
    "print(f2(c1).value)\n",
    "print(f2(c1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:52:53.353887Z",
     "start_time": "2024-08-03T08:52:53.337609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4000,  2.2000], device='cuda:0')\n",
      "tensor([-0.5658, -0.4020, -0.3609, -0.4717, -0.1944], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "LTNObject(value=tensor([-0.5658, -0.4020, -0.3609, -0.4717, -0.1944], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), free_vars=[])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variables\n",
    "\n",
    "LTN variables are sequences of individuals/constants from a domain. Variables are useful in logic to write quantified statements, e.g. $\\forall x\\ P(x)$.\n",
    "\n",
    "Notice that a variable is a sequence and not a set, namely, the same value can occur twice in the sequence.\n",
    "\n",
    "The following defines two variables, $x$ and $y$, with respectively 10 and 5 individuals sampled from normal distributions in $\\mathbb{R}^2$.\n",
    "In LTN, variables need to be labelled (see the arguments `'x'` and `'y'` below). The labels are used internally by LTN to recognize the variables and to\n",
    "implement a lot of other important features.\n",
    "\n",
    "The constructor internally builds an `LTNObject` for the variable. The value will be the given tensor, while the `free_vars` attribute\n",
    "will be the list ['x'] for the first variable, and the list ['y'] for the second variable.\n",
    "\n",
    "The interpretation of the `free_vars` attribute is that it contains the list of the labels of the free variables contained in the `LTNObject`.\n",
    "In logic, a variable is free when it is not quantified by a quantifier, for example the existential quantifier or the universal quantifier.\n",
    "\n",
    "In the case of a variable, it is intuitive to understand that the only free variable is the variable itself. It is for this reason that the `free_vars` attribute\n",
    "contains only the label of the variable itself.\n",
    "\n",
    "For the constants, we did not talk about their `free_vars` attribute since it is an empty list. It is intuitive since a constant\n",
    "is not a variable, so it does not contain free variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 变量\n",
    "\n",
    "LTN变量是从某个域中取出的个体/常量的序列。变量在逻辑中用于编写量化语句，例如 $\\forall x\\ P(x)$。\n",
    "\n",
    "请注意，变量是一个序列而不是一个集合，即同一个值可以在序列中出现两次。\n",
    "\n",
    "下面定义了两个变量，$x$ 和 $y$，它们分别从 $\\mathbb{R}^2$ 中的正态分布中采样出10个和5个个体。\n",
    "在LTN中，变量需要有标签（见下文中的参数 `'x'` 和 `'y'`）。标签在LTN内部用于识别变量以及实现许多其他重要功能。\n",
    "\n",
    "构造函数内部为变量构建了一个 `LTNObject`。值将是给定的张量，而 `free_vars` 属性将是第一个变量的列表 ['x']，以及第二个变量的列表 ['y']。\n",
    "\n",
    "`free_vars` 属性的解释是，它包含了 `LTNObject` 中自由变量的标签列表。\n",
    "**在逻辑中，当一个变量没有被量词（例如存在量词或全称量词）量化时，该变量就是自由的。**\n",
    "\n",
    "对于变量来说，唯一的自由变量就是变量本身。这就是为什么 `free_vars` 属性仅包含变量自身的标签。\n",
    "\n",
    "对于常量，我们没有讨论它们的 `free_vars` 属性，因为它是一个空列表。这是直观的，因为常量不是变量，所以它不包含自由变量。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = ltn.Variable('x', torch.randn((10, 2)))\n",
    "y = ltn.Variable('y', torch.randn((5, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:54:45.983331Z",
     "start_time": "2024-08-03T08:54:45.972707Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating a term/predicate on one variable of $n$ individuals yields $n$ output values, where the $i$-th output value corresponds to the term calculated with the $i$-th individual.\n",
    "\n",
    "Similarly, evaluating a term/predicate on $k$ variables $(x_1,\\dots,x_k)$, with respectively $n_1,\\dots,n_k$ individuals each, yields a result with $n_1 \\times \\dots \\times n_k$ values. The result is organized in a tensor where the first $k$ dimensions can be indexed to retrieve the outcome(s) that correspond to each variable. The tensor is wrapped in an `LTNObject` where the attribute `free_vars` tells which dim of the tensor corresponds to which variable (using the label of the variable).\n",
    "\n",
    "In the example below, the result of $P_4$ has shape 10x5 since x has 10 individuals and y 5 individuals.\n",
    "The first cell of the result is the evaluation of $P_4$ on the first individual of x and first individual of y, the second\n",
    "cell is the evaluation of $P_4$ on the first individual of x and second individual of y, and so forth.\n",
    "\n",
    "Notice the use of the method `shape()` invoked on the `LTNObject` representing the evaluation of $P_4$. This method is a\n",
    "short-cut for `res1.value.shape`.\n",
    "\n",
    "The attribute `free_vars` contains in this case x and y. This is intuitive since no quantifier has been applied.\n",
    "\n",
    "As shown on the last prints, it is possible to access single cells of the tensor containing the result, or the entire result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "评估一个含有 \\(n\\) 个个体的变量上的项/谓词，会产生 \\(n\\) 个输出值，其中第 \\(i\\) 个输出值对应于用第 \\(i\\) 个个体计算的项。\n",
    "\n",
    "类似地，评估含有 \\(k\\) 个变量 $(x_1, \\dots, x_k)$ 的项/谓词，每个变量分别有 $n_1, \\dots, n_k$ 个个体，会产生一个包含 $n_1 \\times \\dots \\times n_k$ 个值的结果。结果被组织在一个张量中，前 \\(k\\) 维可以被索引以检索对应于每个变量的结果。这个张量被封装在一个 `LTNObject` 中，其中属性 `free_vars` 指示张量的哪个维度对应于哪个变量（使用变量的标签）。\n",
    "\n",
    "在下面的例子中，谓词 \\(P_4\\) 的结果形状为 10x5，因为 \\(x\\) 有 10 个个体，\\(y\\) 有 5 个个体。\n",
    "结果的第一个单元是 \\(P_4\\) 在 \\(x\\) 的第一个个体和 \\(y\\) 的第一个个体上的评估值，第二个单元是 \\(P_4\\) 在 \\(x\\) 的第一个个体和 \\(y\\) 的第二个个体上的评估值，以此类推。\n",
    "\n",
    "请注意，使用了方法 `shape()` 来对表示 \\(P_4\\) 的评估的 `LTNObject` 进行调用。这个方法是 `res1.value.shape` 的快捷方式。\n",
    "\n",
    "属性 `free_vars` 在这种情况下包含 \\(x\\) 和 \\(y\\)。这是直观的，因为没有应用量词。\n",
    "\n",
    "如最后的打印所示，可以访问包含结果的张量的单个单元，也可以访问整个结果。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Notice that the outcome is a 2-dimensional tensor where each cell represents the satisfiability of P4 evaluated with each individual in x and in y. # 注意，结果是一个二维张量，其中每个单元格表示 P4 与 x 和 y 中的每个个体计算的可满足性。\n",
    "# P4 = ltn.Predicate(ModelP4()) # 原来的写法会导致报错\n",
    "P4 = ltn.Predicate(ModelP4().to(ltn.device))\n",
    "res1 = P4(x, y)\n",
    "print(res1.shape())\n",
    "print(res1.free_vars) # dynamically added attribute; tells that axis 0 corresponds to x and axis 1 to y # 动态添加的属性；告诉轴 0 对应于 x，轴 1 对应于 y\n",
    "print(res1.value[2, 0]) # gives the result computed with the 3rd individual in x and the 1st individual in y # 给出使用 x 的第三个个体和 y 的第一个个体计算的结果\n",
    "print(res1.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:55:37.395014Z",
     "start_time": "2024-08-03T08:55:37.356886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "['x', 'y']\n",
      "tensor(0.3100, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([[0.3099, 0.3371, 0.4715, 0.5291, 0.4676],\n",
      "        [0.3088, 0.3436, 0.4896, 0.5491, 0.4831],\n",
      "        [0.3100, 0.3471, 0.5024, 0.5657, 0.4954],\n",
      "        [0.3176, 0.3412, 0.4476, 0.4988, 0.4463],\n",
      "        [0.2991, 0.3298, 0.4821, 0.5462, 0.4751],\n",
      "        [0.3179, 0.3472, 0.4806, 0.5351, 0.4762],\n",
      "        [0.2992, 0.3317, 0.4873, 0.5517, 0.4802],\n",
      "        [0.3119, 0.3486, 0.5018, 0.5639, 0.4947],\n",
      "        [0.3211, 0.3483, 0.4770, 0.5294, 0.4730],\n",
      "        [0.3184, 0.3545, 0.5016, 0.5605, 0.4946]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we see the same example but applied to an LTN function instead of an LTN predicate.\n",
    "\n",
    "Notice that the output is no more in [0., 1.] since we have now applied a function.\n",
    "\n",
    "Notice also the changes on the shape. In the case of the predicate we had shape 10x5, since the\n",
    "result contained 10x5 truth values in [0., 1.]. Now, the function does not return a scalar. Instead,\n",
    "it returns a real vector in $\\mathbb{R}^2$. This explains the new shape 10x5x2. For each combination of the two variables\n",
    "we have now a vector in $\\mathbb{R}^2$ returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在，我们看到相同的示例，但应用于 LTN 函数而不是 LTN 谓词。\n",
    "\n",
    "注意，由于我们现在应用的是函数，输出不再在 [0., 1.] 范围内。\n",
    "\n",
    "还要注意形状的变化。在谓词的情况下，我们的形状是 10x5，因为结果包含了 10x5 的真值，这些真值在 [0., 1.] 范围内。现在，函数不返回标量。相反，它返回一个 $\\mathbb{R}^2$ 中的实数向量(这个式子的意思就是这个向量中有两个数，每个数都是实数)。这解释了新的形状 10x5x2。对于两个变量的每个组合，我们现在返回一个 $\\mathbb{R}^2$ 中的向量。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Notice that the last axe(s) correspond to the dimensions of the outcome; # 注意，最后的轴对应于结果的维度；\n",
    "# here, f2 projects to outcomes in R^2, so the outcome has one additional axis of dimension 2. # 这里，f2 投影到 R^2 中的结果，因此结果有一个额外的维度 2。\n",
    "# the output tensor has shape (10, 5, 2) because variable x has 10 individuals, y has 5 individuals, and f1 maps in R^2 # 输出张量的形状为 (10, 5, 2)，因为变量 x 有 10 个个体，y 有 5 个个体，f1 映射到 R^2\n",
    "res2 = f1(x, y)\n",
    "print(res2.shape())\n",
    "print(res2.free_vars)\n",
    "print(res2.value[2,0]) # gives the result calculated with the 3rd individual in x and the 1st individual in y # 给出使用 x 的第三个个体和 y 的第一个个体计算的结果\n",
    "print(res2.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:55:48.168512Z",
     "start_time": "2024-08-03T08:55:48.154725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 2])\n",
      "['x', 'y']\n",
      "tensor([-0.9950,  2.2002], device='cuda:0')\n",
      "tensor([[[-1.5988,  0.3012],\n",
      "         [-1.0251,  0.1124],\n",
      "         [ 0.2601, -0.7936],\n",
      "         [-1.0814, -2.1720],\n",
      "         [-1.3540, -1.6228]],\n",
      "\n",
      "        [[-1.6405,  1.4568],\n",
      "         [-1.0668,  1.2680],\n",
      "         [ 0.2185,  0.3620],\n",
      "         [-1.1231, -1.0164],\n",
      "         [-1.3957, -0.4671]],\n",
      "\n",
      "        [[-0.9950,  2.2002],\n",
      "         [-0.4213,  2.0114],\n",
      "         [ 0.8639,  1.1054],\n",
      "         [-0.4776, -0.2730],\n",
      "         [-0.7502,  0.2763]],\n",
      "\n",
      "        [[ 0.6562, -1.1224],\n",
      "         [ 1.2299, -1.3112],\n",
      "         [ 2.5151, -2.2172],\n",
      "         [ 1.1736, -3.5956],\n",
      "         [ 0.9010, -3.0463]],\n",
      "\n",
      "        [[-0.4849,  1.0355],\n",
      "         [ 0.0888,  0.8467],\n",
      "         [ 1.3740, -0.0592],\n",
      "         [ 0.0325, -1.4376],\n",
      "         [-0.2401, -0.8884]],\n",
      "\n",
      "        [[-2.5436,  0.4161],\n",
      "         [-1.9699,  0.2273],\n",
      "         [-0.6846, -0.6786],\n",
      "         [-2.0262, -2.0571],\n",
      "         [-2.2988, -1.5078]],\n",
      "\n",
      "        [[-0.4820,  1.3242],\n",
      "         [ 0.0917,  1.1354],\n",
      "         [ 1.3769,  0.2294],\n",
      "         [ 0.0354, -1.1490],\n",
      "         [-0.2372, -0.5997]],\n",
      "\n",
      "        [[-1.2199,  2.1822],\n",
      "         [-0.6462,  1.9934],\n",
      "         [ 0.6390,  1.0875],\n",
      "         [-0.7025, -0.2909],\n",
      "         [-0.9751,  0.2583]],\n",
      "\n",
      "        [[-2.6538,  0.0690],\n",
      "         [-2.0801, -0.1198],\n",
      "         [-0.7948, -1.0257],\n",
      "         [-2.1364, -2.4041],\n",
      "         [-2.4090, -1.8549]],\n",
      "\n",
      "        [[-1.8525,  2.1619],\n",
      "         [-1.2788,  1.9731],\n",
      "         [ 0.0065,  1.0672],\n",
      "         [-1.3351, -0.3113],\n",
      "         [-1.6077,  0.2380]]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this last example, we apply a predicate to a variable and a constant.\n",
    "\n",
    "It is interesting to notice that the `free_vars` attribute contains now only y, since only variable y has been given in input.\n",
    "\n",
    "Also, the shape is changed. Now, it is a vector of 5 values. The first cell contains the evaluation of the predicate on\n",
    "$c_1$ and the first individual of $y$, the second cell contains the evaluation of the predicate on $c_1$ and the second individual of $y$, and so forth.\n",
    "Since $y$ has 5 individuals in total, the result is intuitively a vector containing 5 truth values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在最后一个例子中，我们将谓词应用于一个变量和一个常量。\n",
    "\n",
    "有趣的是，`free_vars` 属性现在仅包含 y，因为输入中只有变量 y。\n",
    "\n",
    "此外，形状也发生了变化。现在，它是一个包含 5 个值的向量。第一个单元格包含谓词在 $c_1$ 和 y 的第一个个体上的评估结果，第二个单元格包含谓词在 $c_1$ 和 y 的第二个个体上的评估结果，依此类推。\n",
    "由于 y 总共有 5 个个体，因此结果直观上是一个包含 5 个真值的向量。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "res3 = P4(c1, y)\n",
    "print(res3.shape()) # Notice that no axis is associated to a constant. The output has shape (5,) because variable y has # 注意，没有轴与常量相关联。输出的形状为 (5,)，因为变量 y 有 5 个个体\n",
    "# 5 individuals # 5 个个体\n",
    "print(res3.free_vars)\n",
    "print(res3.value[0]) # gives the result calculated with c1 and the 1st individual in y # 给出使用 c1 和 y 的第一个个体计算的结果\n",
    "print(res3.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T08:55:53.593161Z",
     "start_time": "2024-08-03T08:55:53.571797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "['y']\n",
      "tensor(0.3344, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([0.3344, 0.3803, 0.5439, 0.6167, 0.5471], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables made of trainable constants\n",
    "\n",
    "In LTN, it is possible to define variables which individuals are trainable constants. This is really useful in embedding learning\n",
    "tasks, where the individuals of a variable could be embeddings that have to be learned.\n",
    "\n",
    "In order to better understand how trainable variables can be used in LTN, it is kindly suggested to carefully read the `Smokes-friends-cancer` example, where variables\n",
    "made of trainable constants are used.\n",
    "\n",
    "In this example, two trainable constants are declared. Then, an LTN variable is built using these constants.\n",
    "The first individual of the variable will be the first constant, while the second individual will be the second constant.\n",
    "\n",
    "Notice that after $P_2$ has been evaluated on $x$, both individuals of $x$ will have a `grad_fn` attribute. This means the gradient\n",
    "has passed through the learnable constants by using the predicate.\n",
    "\n",
    "It is important to note that in the declaration of the variables we have given the values of the constants and not the constants themselves.\n",
    "This is why the two constants have `grad_fn` set to `None` after the predicate is applied. Since we have given the value, and\n",
    "not the constant itself, the constants will remain untouched after the application of the predicate. Specifically, we have given the values of the constants to the variable since\n",
    "LTN variables only accepts PyTorch tensors as values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "在 LTN 中，可以定义由可训练常量组成的变量。这在嵌入学习任务中非常有用，其中变量的个体可以是需要学习的嵌入。\n",
    "\n",
    "为了更好地理解如何在 LTN 中使用可训练变量，建议仔细阅读 `Smokes-friends-cancer` 示例，其中使用了由可训练常量组成的变量。\n",
    "\n",
    "在这个示例中，声明了两个可训练常量。然后，使用这些常量构建了一个 LTN 变量。\n",
    "变量的第一个个体将是第一个常量，而第二个个体将是第二个常量。\n",
    "\n",
    "请注意，在 $P_2$ 对 $x$ 进行评估后，$x$ 的两个个体都将具有 `grad_fn` 属性。这意味着梯度已经通过谓词传递到了可训练常量。\n",
    "\n",
    "重要的是要注意，在变量的声明中，我们给出了常量的值，而不是常量本身。这就是为什么在应用谓词后，这两个常量的 `grad_fn` 被设置为 `None`。因为我们给出了值，而不是常量本身，所以常量在应用谓词后将保持不变。具体来说，我们将常量的值赋予了变量，因为 LTN 变量只接受 PyTorch 张量作为值。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]), trainable=True)\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]), trainable=True)\n",
    "\n",
    "# PyTorch will keep track of the gradients between c1, c2 and x. # PyTorch 将跟踪 c1、c2 和 x 之间的梯度。\n",
    "# Read tutorial 3 for more details. # 有关更多详细信息，请阅读教程 3。\n",
    "x = ltn.Variable('x', torch.stack([c1.value, c2.value]))\n",
    "res = P2(x)\n",
    "print(res.value)\n",
    "\n",
    "print(\"---------------\")\n",
    "\n",
    "print(x.value[0])\n",
    "print(x.value[1])\n",
    "\n",
    "print(c1.value.grad_fn)\n",
    "print(c2.value.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T10:11:29.185875Z",
     "start_time": "2024-08-03T10:11:29.127085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3661, 0.4114], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "---------------\n",
      "tensor([2.1000, 3.0000], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([4.5000, 0.8000], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
