{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class single label classification\n",
    "\n",
    "The natural extension of binary classification is a multi-class classification task.\n",
    "\n",
    "We first approach multi-class single-label classification, which makes the assumption that each example is assigned\n",
    "to one and only one label.\n",
    "For illustration purposes, we use the Iris flower data set, which consists of a classification into three\n",
    "mutually-exclusive classes. Let us assume these classes are $A$, $B$, and $C$.\n",
    "\n",
    "While one could train three unary\n",
    "predicates $A(x)$, $B(x)$ and $C(x)$, it turns out to be more effective if this problem is modelled by a single\n",
    "binary predicate $P(x,l)$, where $l$ is a variable denoting a multi-class label, in this case classes $A$, $B$, or $C$.\n",
    "This syntax allows one to write statements quantifying over the classes, e.g. $\\forall x(\\exists l(P(x, l)))$, which states\n",
    "that each example should be assigned at least one class.\n",
    "\n",
    "Note that since the classes are mutually-exclusive in this case, the output layer of the $MLP$ representing predicate $P(x,l)$\n",
    "will be a softmax layer, instead of a sigmoid function, to learn the probability of $A$, $B$, and $C$.\n",
    "\n",
    "For this specific task, LTN uses the following language and grounding:\n",
    "\n",
    "**Domains:**\n",
    "- $items$, denoting the examples from the Iris flower data set;\n",
    "- $labels$, denoting the class labels.\n",
    "\n",
    "**Variables:**\n",
    "- $x_A, x_B, x_C$ for the positive examples of classes $A$, $B$, and $C$;\n",
    "- $x$ for all examples;\n",
    "- $D(x_A) = D(x_B) = D(x_C) = D(x) = items$.\n",
    "\n",
    "**Constants:**\n",
    "- $l_A, l_B, l_C$, the labels of classes $A$ (Iris setosa), $B$ (Iris virginica), $C$ (Iris versicolor), respectively;\n",
    "- $D(l_A) = D(l_B) = D(l_C) = labels$.\n",
    "\n",
    "**Predicates:**\n",
    "- $P(x,l)$ denoting the fact that item $x$ is classified as $l$;\n",
    "- $D_{in}(P) = items,labels$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "- $\\forall x_A P(x_A, l_A)$: all the examples of class $A$ should have label $l_A$;\n",
    "- $\\forall x_B P(x_B, l_B)$: all the examples of class $B$ should have label $l_B$;\n",
    "- $\\forall x_C P(x_C, l_C)$: all the examples of class $C$ should have label $l_C$.\n",
    "\n",
    "Notice that rules about exclusiveness such as $\\forall x (P(x, l_A) \\implies (\\lnot P(x, l_B) \\land \\lnot P(x, l_C)))$ are not included since such constraints are already imposed by the\n",
    "grounding of $P$ below, more specifically by the *softmax* function.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(items)=\\mathbb{R}^{4}$, items are described by 4 features: the length and the width of the sepals and\n",
    "petals, in centimeters;\n",
    "- $\\mathcal{G}(labels)=\\mathbb{N}^{3}$, we use a one-hot encoding to represent classes;\n",
    "- $\\mathcal{G}(x_A) \\in \\mathbb{R}^{m_1 \\times 4}$, that is, $\\mathcal{G}(x_A)$ is a sequence of $m_1$ examples of class $A$;\n",
    "- $\\mathcal{G}(x_B) \\in \\mathbb{R}^{m_2 \\times 4}$, that is, $\\mathcal{G}(x_B)$ is a sequence of $m_2$ examples of class $B$;\n",
    "- $\\mathcal{G}(x_C) \\in \\mathbb{R}^{m_3 \\times 4}$, that is, $\\mathcal{G}(x_C)$ is a sequence of $m_3$ examples of class $C$;\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{(m_1+m_2+m_3) \\times 4}$, that is, $\\mathcal{G}(x)$ is a sequence of all the examples;\n",
    "- $\\mathcal{G}(l_A) = [1, 0, 0]$, $\\mathcal{G}(l_B) = [0, 1, 0]$, $\\mathcal{G}(l_C) = [0, 0, 1]$;\n",
    "- $\\mathcal{G}(P \\mid \\theta): x,l \\mapsto l^\\top \\cdot \\operatorname{softmax}\\left(\\operatorname{MLP}_{\\theta}(x)\\right)$, where $MLP$\n",
    "has three output neurons corresponding to as many classes, and $\\cdot$ denotes the dot product as a way of selecting an\n",
    "output for $\\mathcal{G}(P \\mid \\theta)$. In fact, multiplying the $MLP$’s output by the one-hot vector $l^\\top$ gives the probability\n",
    "corresponding to the class denoted by $l$.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import the dataset.\n",
    "\n",
    "The Iris flower dataset has three classes with 50 examples each. Every example is represented by 4 features. The dataset\n",
    "is already subdivided into train and test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 多类单标签分类\n",
    "\n",
    "二分类的自然扩展是多类分类任务。\n",
    "\n",
    "我们首先研究多类单标签分类，这假设每个样本只被分配一个且只有一个标签。\n",
    "为了说明，我们使用鸢尾花数据集，该数据集包含三个互斥的类别。我们假设这些类别为 $A$, $B$ 和 $C$。\n",
    "\n",
    "虽然可以训练三个一元谓词 $A(x)$, $B(x)$ 和 $C(x)$，但如果用一个二元谓词 $P(x,l)$ 来建模这个问题会更有效，其中 $l$ 是表示多类标签的变量，在这个例子中是类别 $A$, $B$ 或 $C$。\n",
    "这种语法允许我们编写对类别进行量化的语句，例如 $\\forall x(\\exists l(P(x, l)))$，这表示每个样本应至少被分配一个类别。\n",
    "\n",
    "注意，由于在这种情况下类别是互斥的，表示谓词 $P(x,l)$ 的多层感知器（MLP）的输出层将是一个 softmax 层，而不是 sigmoid 函数，以学习 $A$, $B$ 和 $C$ 的概率。\n",
    "\n",
    "前面第一个例子中之所以可以使用sigmoid，因为他谓词那里的模型设置最后只需要输出一个值，他是一个二分类问题，这个值代表的是这个点是某一个类的置信度吧，大于0.5就是这个类，小于0.5就不是这个类，而不需要像像多分类问题那样，要求输出的结果和是1，这里的输出就是是每个类的概率，所以要用softmax。（自）\n",
    "\n",
    "对于这个特定任务，LTN 使用以下语言和基础：\n",
    "\n",
    "**领域:**\n",
    "- $items$，表示来自鸢尾花数据集的样本；\n",
    "- $labels$，表示类别标签。\n",
    "\n",
    "**变量:**\n",
    "- $x_A, x_B, x_C$ 分别表示类别 $A$, $B$ 和 $C$ 的正样本；\n",
    "- $x$ 表示所有样本；\n",
    "- $D(x_A) = D(x_B) = D(x_C) = D(x) = items$。\n",
    "\n",
    "**常量:**\n",
    "- $l_A, l_B, l_C$ 分别表示类别 $A$（山鸢尾）, $B$（维吉尼亚鸢尾）, $C$（杂色鸢尾）的标签；\n",
    "- $D(l_A) = D(l_B) = D(l_C) = labels$。\n",
    "\n",
    "**谓词:**\n",
    "- $P(x,l)$ 表示样本 $x$ 被分类为 $l$；\n",
    "- $D_{in}(P) = items,labels$。\n",
    "\n",
    "**公理:**\n",
    "- - (自)知识库中的这些逻辑表达式的置信度希望越来越高\n",
    "\n",
    "- $\\forall x_A P(x_A, l_A)$: 类别 $A$ 的所有样本应有标签 $l_A$；\n",
    "- $\\forall x_B P(x_B, l_B)$: 类别 $B$ 的所有样本应有标签 $l_B$；\n",
    "- $\\forall x_C P(x_C, l_C)$: 类别 $C$ 的所有样本应有标签 $l_C$。\n",
    "\n",
    "注意，不包含诸如 $\\forall x (P(x, l_A) \\implies (\\lnot P(x, l_B) \\land \\lnot P(x, l_C)))$ 这样的互斥规则，因为这些约束已经由 $P$ 的基础设定所施加，更具体地说是由 *softmax* 函数施加的。\n",
    "\n",
    "**基础设定:**\n",
    "- $\\mathcal{G}(items)=\\mathbb{R}^{4}$，样本由4个特征描述：萼片和花瓣的长度和宽度，以厘米为单位；\n",
    "- $\\mathcal{G}(labels)=\\mathbb{N}^{3}$，我们使用独热编码表示类别；\n",
    "- $\\mathcal{G}(x_A) \\in \\mathbb{R}^{m_1 \\times 4}$，即 $\\mathcal{G}(x_A)$ 是类别 $A$ 的 $m_1$ 个样本序列；\n",
    "- $\\mathcal{G}(x_B) \\in \\mathbb{R}^{m_2 \\times 4}$，即 $\\mathcal{G}(x_B)$ 是类别 $B$ 的 $m_2$ 个样本序列；\n",
    "- $\\mathcal{G}(x_C) \\in \\mathbb{R}^{m_3 \\times 4}$，即 $\\mathcal{G}(x_C)$ 是类别 $C$ 的 $m_3$ 个样本序列；\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{(m_1+m_2+m_3) \\times 4}$，即 $\\mathcal{G}(x)$ 是所有样本的序列；\n",
    "- $\\mathcal{G}(l_A) = [1, 0, 0]$, $\\mathcal{G}(l_B) = [0, 1, 0]$, $\\mathcal{G}(l_C) = [0, 0, 1]$；\n",
    "- $\\mathcal{G}(P \\mid \\theta): x,l \\mapsto l^\\top \\cdot \\operatorname{softmax}\\left(\\operatorname{MLP}_{\\theta}(x)\\right)$，其中 $MLP$ 有三个对应于多个类别的输出神经元，$\\cdot$ 表示点积，用于选择 $\\mathcal{G}(P \\mid \\theta)$ 的输出。实际上，将 $MLP$ 的输出与独热向量 $l^\\top$ 相乘得到对应于类别的概率。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "现在，让我们导入数据集。\n",
    "\n",
    "鸢尾花数据集有三个类别，每个类别50个样本。每个样本由4个特征表示。数据集已经分为训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 相关解释在ex2.md的## 1中\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"datasets/iris_training.csv\") # 返回值类型是 DataFrame，具体来说，是<class 'pandas.core.frame.DataFrame'>，其实就是一个二维表格\n",
    "test_data = pd.read_csv(\"datasets/iris_test.csv\")\n",
    "\n",
    "print(\"type(train_data\\n):\", type(train_data))\n",
    "print(\"before_pop_train_data\\n:\", train_data[:5])\n",
    "\n",
    "train_labels = train_data.pop(\"species\")\n",
    "test_labels = test_data.pop(\"species\")\n",
    "print(\"after_pop_train_data\\n:\", train_data[:5]) # pop() 方法会删除指定列，并返回删除的列\n",
    "\n",
    "print(\"type(train_labels\\n):\", type(train_labels)) # 返回值类型是 Series，具体来说，是<class 'pandas.core.series.Series'>，其实就是一个一维数组\n",
    "\n",
    "train_data = torch.tensor(train_data.to_numpy()).float()\n",
    "test_data = torch.tensor(test_data.to_numpy()).float()\n",
    "train_labels = torch.tensor(train_labels.to_numpy()).long()\n",
    "test_labels = torch.tensor(test_labels.to_numpy()).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-08T01:55:21.926228Z",
     "start_time": "2024-08-08T01:55:17.214352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_data\n",
      "): <class 'pandas.core.frame.DataFrame'>\n",
      "before_pop_train_data\n",
      ":    sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           6.4          2.8           5.6          2.2        2\n",
      "1           5.0          2.3           3.3          1.0        1\n",
      "2           4.9          2.5           4.5          1.7        2\n",
      "3           4.9          3.1           1.5          0.1        0\n",
      "4           5.7          3.8           1.7          0.3        0\n",
      "after_pop_train_data\n",
      ":    sepal_length  sepal_width  petal_length  petal_width\n",
      "0           6.4          2.8           5.6          2.2\n",
      "1           5.0          2.3           3.3          1.0\n",
      "2           4.9          2.5           4.5          1.7\n",
      "3           4.9          3.1           1.5          0.1\n",
      "4           5.7          3.8           1.7          0.3\n",
      "type(train_labels\n",
      "): <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mircocrift\\AppData\\Local\\Temp\\ipykernel_16400\\3935815502.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T01:55:21.974111Z",
     "start_time": "2024-08-08T01:55:21.928224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"type(train_labels\\n):\", type(train_labels))\n",
    "print(\"train_data\\n:\", train_data[:5])\n",
    "print(\"train_data.shape:\\n\", train_data.shape)\n",
    "print(\"test_data.shape:\\n\", test_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_labels\n",
      "): <class 'torch.Tensor'>\n",
      "train_data\n",
      ": tensor([[6.4000, 2.8000, 5.6000, 2.2000],\n",
      "        [5.0000, 2.3000, 3.3000, 1.0000],\n",
      "        [4.9000, 2.5000, 4.5000, 1.7000],\n",
      "        [4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [5.7000, 3.8000, 1.7000, 0.3000]])\n",
      "train_data.shape:\n",
      " torch.Size([120, 4])\n",
      "test_data.shape:\n",
      " torch.Size([30, 4])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define predicate $P$, constants $l_A$, $l_B$, $l_C$,\n",
    "universal quantifier, and the `SatAgg` operator.\n",
    "\n",
    "For the quantifier, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For predicate $P$, we have two models. The first one implements an $MLP$ which outputs the logits for the three classes of\n",
    "the Iris flower dataset, given an example $x$ in input. The second model takes as input a labelled example $(x,l)$, it computes the logits\n",
    "using the first model and then returns the prediction (*softmax*) for class $l$.\n",
    "\n",
    "We need two separated models because we need both logits and probabilities. Logits are used to compute the classification\n",
    "accuracy, while probabilities are interpreted as truth values to compute the satisfaction level of the knowledge base.\n",
    "\n",
    "The constants $l_A$, $l_B$, and $l_C$, represent the one-hot labels for the three classes, as we have already seen in the\n",
    "definition of the grounding for this task.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LTN 设置\n",
    "\n",
    "为了定义我们的知识库（公理），我们需要定义谓词 $P$、常量 $l_A$、$l_B$、$l_C$、全称量词和 `SatAgg` 运算符。\n",
    "\n",
    "对于量词，我们使用稳定乘积配置（在教程中可以看到）。\n",
    "\n",
    "对于谓词 $P$，我们有两个模型。第一个模型实现了一个 $MLP$，它输出鸢尾花数据集三个类别的逻辑回归值（logits ，# todo:具体指什么？），给定输入示例 $x$。第二个模型接收一个标记的示例 $(x,l)$ 作为输入，使用第一个模型计算逻辑回归值，然后返回类 $l$ 的预测值（*softmax*）。\n",
    "\n",
    "我们需要两个独立的模型，因为我们需要逻辑回归值和概率。逻辑回归值用于计算分类准确性，而概率被解释为真值(truth values)，用于计算知识库的满意度。\n",
    "\n",
    "常量 $l_A$、$l_B$ 和 $l_C$ 表示三个类别的独热编码标签，就像我们在这个任务的基础定义中看到的那样。\n",
    "\n",
    "`SatAgg` 使用 `pMeanError` 聚合器定义。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 相关解释在2.md的## 2中\n",
    "import ltn\n",
    "\n",
    "# we define the constants # 我们定义常量\n",
    "l_A = ltn.Constant(torch.tensor([1, 0, 0]))\n",
    "l_B = ltn.Constant(torch.tensor([0, 1, 0]))\n",
    "l_C = ltn.Constant(torch.tensor([0, 0, 1]))\n",
    "\n",
    "# we define predicate P # 我们定义谓词P\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model returns the logits for the classes given an input example. It does not compute the softmax, so the output are not normalized.\n",
    "    This is done to separate the accuracy computation from the satisfaction level computation. Go through the example to understand it.\n",
    "    该模型在给定输入示例的情况下返回类别的逻辑回归值。它不会计算 softmax，因此输出未进行归一化（normalized）处理。\n",
    "    这样做是为了将准确度计算与满意度计算分开。请通过示例来理解这一点。\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes=(4, 16, 16, 8, 3)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i]) for i in range(1, len(layer_sizes))]) # torch.nn.Linear(in_features, out_features)，返回值是一个线性层，输入特征数为in_features，输出特征数为out_features\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Method which defines the forward phase of the neural network for our multi class classification task.\n",
    "        In particular, it returns the logits for the classes given an input example.\n",
    "\n",
    "        :param x: the features of the example\n",
    "        :param training: whether the network is in training mode (dropout applied) or validation mode (dropout not applied)\n",
    "        :return: logits for example x\n",
    "        定义了神经网络前向传递阶段的方法，用于我们的多类别分类任务。特别是，它返回给定输入示例的类别逻辑回归值（the logits）。\n",
    "\n",
    "        :param x: 示例的特征\n",
    "        :param training: 指示网络是否处于训练模式（应用dropout）或验证模式（不应用dropout）\n",
    "        :return: 示例 x 的逻辑回归值\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers[:-1]: # 去掉最后一层\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x) # 最后一层不用激活函数\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    这个模型内部包含一个逻辑回归模型，即一个给定输入示例 $ x $ 计算类别逻辑回归值（logits）的模型。这个模型的理念是将逻辑回归值和概率分开。逻辑回归模型返回一个示例的逻辑回归值，而这个模型返回基于逻辑回归模型计算的概率。\n",
    "\n",
    "    具体来说，它接收一个示例 $ x $ 和一个类别标签 $ l $ 作为输入。它对 $ x $ 应用逻辑回归模型以获取逻辑回归值。然后，它应用 softmax 函数来获取每个类别的概率。最后，它只返回与给定类别 $ l $ 相关的概率。\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training) # :param training: 指示网络是否处于训练模式（应用dropout）或验证模式（不应用dropout）\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n",
    "\n",
    "# mlp = MLP() # 原来的写法会报错\n",
    "# P = ltn.Predicate(LogitsToPredicate(mlp)) # 原来的写法会报错\n",
    "mlp = MLP().to(ltn.device)\n",
    "P = ltn.Predicate(LogitsToPredicate(mlp).to(ltn.device))\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg # 我们定义连接词、量词和SatAgg\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-08T01:55:22.203236Z",
     "start_time": "2024-08-08T01:55:21.976108Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data.\n",
    "\n",
    "Then, we define functions to evaluate the model performances. The model is evaluated on the test set using the following metrics:\n",
    "- the satisfaction level of the knowledge base: measure the ability of LTN to satisfy the knowledge;\n",
    "- the classification accuracy: measure the quality of the predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 工具类和函数\n",
    "\n",
    "现在，我们需要定义一些实用的类和函数。\n",
    "\n",
    "我们定义了一个标准的 PyTorch 数据加载器，它以数据集为输入，并返回数据批次的生成器。\n",
    "特别地，我们需要一个用于训练数据的数据加载器实例和一个用于测试数据的数据加载器实例。\n",
    "\n",
    "然后，我们定义了一些函数来评估模型的性能。模型在测试集上使用以下指标进行评估：\n",
    "- 知识库的满意度水平：衡量 LTN 满足知识的能力；\n",
    "- 分类准确率：衡量预测的质量。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model # 这是一个标准的PyTorch DataLoader，用于加载数据集以训练和测试模型\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield data, labels\n",
    "\n",
    "\n",
    "# define metrics for evaluation of the model # 定义用于评估模型的指标\n",
    "\n",
    "# it computes the overall satisfaction level on the knowledge base using the given data loader (train or test) # 它使用给定的数据加载器（train或test）计算知识库的整体满意度水平\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0]) # 条件索引：用于根据条件选择数组或张量的元素。data[labels == 0]：数据子集，标签为 0 的数据。这里的 0 是指类别 A。\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1])\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A)),\n",
    "            Forall(x_B, P(x_B, l_B)),\n",
    "            Forall(x_C, P(x_C, l_C))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "# it computes the overall accuracy of the predictions of the trained model using the given data loader # 它使用给定的数据加载器计算经过训练的模型的预测的整体准确性\n",
    "# (train or test) # 训练或测试\n",
    "def compute_accuracy(loader):\n",
    "    mean_accuracy = 0.0 # 初始化一个浮点型变量 mean_accuracy，用于累加每个批次的准确率\n",
    "    for data, labels in loader:\n",
    "        # predictions = mlp(data).detach().numpy() # 原来的写法会报错\n",
    "        predictions = mlp(data.to(ltn.device)).detach().cpu().numpy() # 返回的结果是logits，还没有经过softmax处理。利用logits计算准确率。\n",
    "        predictions = np.argmax(predictions, axis=1) # 返回沿轴axis最大值的索引，axis=1 表示按行取最大值的索引。沿指定轴（axis=1）找到最大值的索引。\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)\n",
    "\n",
    "# create train and test loader # 创建训练和测试加载器\n",
    "train_loader = DataLoader(train_data, train_labels, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, test_labels, 64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-08T01:55:23.745695Z",
     "start_time": "2024-08-08T01:55:22.205237Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T01:55:35.855020Z",
     "start_time": "2024-08-08T01:55:35.841889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for data, labels in train_loader:\n",
    "    i=0\n",
    "    if i==0:\n",
    "        print(\"data:\\n\", data[:5])\n",
    "        print(\"labels:\\n\", labels[:5])\n",
    "        i+=1\n",
    "    print(\"data.shape:\", data.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      " tensor([[5.9000, 3.2000, 4.8000, 1.8000],\n",
      "        [4.8000, 3.1000, 1.6000, 0.2000],\n",
      "        [6.1000, 3.0000, 4.9000, 1.8000],\n",
      "        [6.7000, 3.1000, 5.6000, 2.4000],\n",
      "        [5.5000, 3.5000, 1.3000, 0.2000]])\n",
      "labels:\n",
      " tensor([1, 0, 2, 2, 0])\n",
      "data.shape: torch.Size([64, 4])\n",
      "labels.shape: torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function with $\\mathcal{K}=\\{\\forall x_A P(x_A, l_A),\\forall x_B P(x_B, l_B),\\forall x_C P(x_C, l_C)\\}$\n",
    "is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "In the following, we learn our LTN in the multi-class single-label classification task using the satisfaction of the knowledge base as\n",
    "an objective. In other words, we want to learn the parameters $\\theta$ of binary predicate $P$ in such a way the three\n",
    "axioms in the knowledge base are maximally satisfied. We train our model for 500 epochs and use the `Adam` optimizer.\n",
    "\n",
    "The following figure shows the LTN computational graph for this specific task.\n",
    "\n",
    "![Computational graph](/examples/images/multi-class-single-label-classification.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 学习\n",
    "\n",
    "让我们定义 $D$ 为所有样本的数据集。目标函数 $\\mathcal{K}=\\{\\forall x_A P(x_A, l_A),\\forall x_B P(x_B, l_B),\\forall x_C P(x_C, l_C)\\}$ 的形式如下：$\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$。\n",
    "\n",
    "实际上，优化器使用以下损失函数：\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "其中 $B$ 是从 $D$ 中采样的一个小批量。\n",
    "\n",
    "在接下来的部分中，我们使用知识库的满足度作为目标来学习我们的 LTN 在多类单标签分类任务中的表现。换句话说，我们希望学习二元谓词 $P$ 的参数 $\\theta$，以使知识库中的三个公理最大程度地得到满足。我们将模型训练500个周期，并使用 `Adam` 优化器。\n",
    "\n",
    "下图展示了针对这一特定任务的 LTN 计算图。\n",
    "\n",
    "![计算图](/examples/images/multi-class-single-label-classification.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2024/08/2024_08_07__17_49_15_a71525.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上面的计算图，除了直接看成是所有的数据，也可以看成是训练那里一个epoch中的一个batch中的数据。更形象一些好像。\n",
    "\n",
    "相关的解释在ex2.md的## 3中"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader): # 应该就是嵌套解包\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data # 我们使用当前批次数据对变量进行实例化\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0]) # class A examples # A类示例\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1]) # class B examples # B类示例\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2]) # class C examples # C类示例\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A, training=True)),\n",
    "            Forall(x_B, P(x_B, l_B, training=True)),\n",
    "            Forall(x_C, P(x_C, l_C, training=True))\n",
    "        ) # 通过看源码会发现，P(x_A, l_A, training=True)应该是调用了Predicate类的__call__方法，这个方法会调用Predicate类的forward方法，这个方法里面又会调用LogitsToPredicate类的__call__方法，这个方法里面会调用LogitsToPredicate类的forward方法，这个方法里面又会调用MLP类的__call__方法，这个方法里面又会调用MLP类的forward方法……这个过程中发生了参数传递，比如training这个参数，这个参数本身在Predicate类中是没有的，在Predicate类的代码中，是以**kwargs的形式接收的，然后传递给LogitsToPredicate类的forward方法，然后传递给MLP类的forward方法……\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training # 我们每20个时代打印指标\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader), compute_accuracy(test_loader)))\n",
    "        # 会发现输出的train_loss是当前这个epoch的平均每个batch的loss\n",
    "        # 会发现当使用compute_sat_level函数时，将一个dataloader传递给这个函数，这个函数会通过多个batch将这个dataloader中的所有数据都过一遍，计算平均每个batch的满意度\n",
    "        # 会发现当使用compute_accuracy函数时，将一个dataloader传递给这个函数，这个函数会通过多个batch将这个dataloader中的所有数据都过一遍，计算平均每个batch的准确率\n",
    "        # 随着epoch的增加，P这个谓词的参数会不断更新，从而使得train_loss逐渐减小，compute_sat_level(train_loader)逐渐增大，compute_sat_level(test_loader)逐渐增大，compute_accuracy(train_loader)逐渐增大，compute_accuracy(test_loader)逐渐增大"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-08T02:18:29.313016Z",
     "start_time": "2024-08-08T02:18:16.559669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.6608 | Train Sat 0.352 | Test Sat 0.351 | Train Acc 0.682 | Test Acc 0.533\n",
      " epoch 20 | loss 0.5984 | Train Sat 0.422 | Test Sat 0.419 | Train Acc 0.711 | Test Acc 0.533\n",
      " epoch 40 | loss 0.4911 | Train Sat 0.544 | Test Sat 0.542 | Train Acc 0.752 | Test Acc 0.600\n",
      " epoch 60 | loss 0.4353 | Train Sat 0.617 | Test Sat 0.617 | Train Acc 0.942 | Test Acc 0.933\n",
      " epoch 80 | loss 0.3776 | Train Sat 0.667 | Test Sat 0.668 | Train Acc 0.967 | Test Acc 0.933\n",
      " epoch 100 | loss 0.3304 | Train Sat 0.729 | Test Sat 0.735 | Train Acc 0.982 | Test Acc 1.000\n",
      " epoch 120 | loss 0.2778 | Train Sat 0.792 | Test Sat 0.803 | Train Acc 0.983 | Test Acc 1.000\n",
      " epoch 140 | loss 0.2177 | Train Sat 0.832 | Test Sat 0.846 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 160 | loss 0.2137 | Train Sat 0.846 | Test Sat 0.864 | Train Acc 0.975 | Test Acc 0.967\n",
      " epoch 180 | loss 0.1987 | Train Sat 0.856 | Test Sat 0.875 | Train Acc 0.982 | Test Acc 0.967\n",
      " epoch 200 | loss 0.1599 | Train Sat 0.875 | Test Sat 0.879 | Train Acc 0.983 | Test Acc 1.000\n",
      " epoch 220 | loss 0.1479 | Train Sat 0.866 | Test Sat 0.882 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 240 | loss 0.1792 | Train Sat 0.876 | Test Sat 0.883 | Train Acc 0.984 | Test Acc 0.967\n",
      " epoch 260 | loss 0.1311 | Train Sat 0.885 | Test Sat 0.884 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 280 | loss 0.1484 | Train Sat 0.874 | Test Sat 0.872 | Train Acc 0.991 | Test Acc 0.967\n",
      " epoch 300 | loss 0.1404 | Train Sat 0.892 | Test Sat 0.873 | Train Acc 0.991 | Test Acc 0.967\n",
      " epoch 320 | loss 0.1421 | Train Sat 0.872 | Test Sat 0.874 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 340 | loss 0.1287 | Train Sat 0.912 | Test Sat 0.880 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 360 | loss 0.1492 | Train Sat 0.876 | Test Sat 0.876 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 380 | loss 0.1460 | Train Sat 0.878 | Test Sat 0.877 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 400 | loss 0.1267 | Train Sat 0.890 | Test Sat 0.874 | Train Acc 0.982 | Test Acc 0.967\n",
      " epoch 420 | loss 0.1224 | Train Sat 0.908 | Test Sat 0.875 | Train Acc 0.992 | Test Acc 0.967\n",
      " epoch 440 | loss 0.1109 | Train Sat 0.903 | Test Sat 0.872 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 460 | loss 0.1514 | Train Sat 0.885 | Test Sat 0.874 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 480 | loss 0.1220 | Train Sat 0.880 | Test Sat 0.872 | Train Acc 0.984 | Test Acc 0.967\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that variables $x_A$, $x_B$, and $x_C$ are grounded batch by batch with new data arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg` takes as input the three axioms and returns one truth value which can be interpreted as the satisfaction\n",
    "level of the knowledge base.\n",
    "\n",
    "Note that after 80 epochs the test accuracy is around 1. This shows the power of LTN in learning\n",
    "the multi-class single-label classification task only using the satisfaction of a knowledge base as an objective."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "好的，以下是您提供内容的中文翻译：\n",
    "\n",
    "注意，变量 $ x_A $、$ x_B $ 和 $ x_C $ 是通过数据加载器逐批接收新数据来进行基础化的。这正是我们所说的 $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$，其中 $ B $ 是由数据加载器抽样的一个小批次。\n",
    "\n",
    "还请注意，`SatAgg` 接受三个公理作为输入，并返回一个可以解释为知识库满足度的真值。\n",
    "\n",
    "请注意，在经过80个周期后，测试准确率大约为1。这显示了LTN在仅使用知识库的满足度作为目标来学习多类别单标签分类任务中的强大功能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
