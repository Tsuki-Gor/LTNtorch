{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Semi-supervised pattern recognition\n",
    "\n",
    "Let us now explore two, more elaborate, classification tasks, which showcase the\n",
    "benefit of using logical reasoning alongside machine learning.\n",
    "\n",
    "The tasks consist in predicting the sum of two numbers given in input. We have the images for these numbers but we do not\n",
    "know their classes, namely their labels. The only thing we know is their sum. In particular, the first task is simpler\n",
    "and consists in predicting the sum of two single-digit numbers. The second one is more complicated since it consists in\n",
    "predicting the sum of two multi-digits numbers.\n",
    "\n",
    "To create our dataset, we use the popular MNIST dataset, which contains labelled manuscripted digits.\n",
    "\n",
    "**Single digits addition:**\n",
    "consider the predicate $addition(X, Y, n)$, where $X$ and $Y$ are\n",
    "images of digits, and $n$ is a natural number corresponding to the sum of these digits. This predicate should return an\n",
    "estimate of the validity of the addition. For instance, $addition(img(8), img(3), 11)$ is a\n",
    "valid addition, while $addition(img(3), img(3), 5)$ is not. In this example, $img(x)$ means \"an image of a $x$\", where $x$ is\n",
    "a digit label. It has not to be confused with a logical function.\n",
    "\n",
    "**Multi digits addition:**\n",
    "the experiment is extended to numbers with more than one\n",
    "digit. Consider the predicate $addition([img(X_1), img(X_2)],[img(Y_1), img(Y_2)], n)$, where $[img(X_1), img(X_2)]$ and\n",
    "$[img(Y_1), img(Y_2)]$ are lists of images of digits, representing two multi-digit numbers, and $n$ is a natural\n",
    "number corresponding to the sum of the two multi-digit numbers. For instance,\n",
    "$addition([img(2), img(0)], [img(1), (7)], 37)$ is a valid addition, while $addition([img(5), img(4)], [img(9), img(0)], 50)$\n",
    "is not.\n",
    "\n",
    "A natural Neural-Symbolic approach is to seek to learn a single digit classifier and benefit from knowledge readily available about the properties of the addition in this case.For instance, suppose that a predicate $digit(x,d)$ gives the likelihood of an image $x$ being of digit $d$. A definition for $addition(img(3), img(8), 11)$ in LTN is:\n",
    "\n",
    "$\\exists d_1, d_2 : d_1 + d_2 = 11 \\text{ } (digit(img(3), d_1) \\land digit(img(8), d_2))$.\n",
    "\n",
    "The above task is made more complicated by not providing labels for the\n",
    "single digit images during training. Instead, training takes place on pairs of images\n",
    "with labels made available for the result only, that is, the sum of the individual\n",
    "labels. The single digit classifier is not explicitly trained by itself. Its output is a\n",
    "latent information which is used by the logic. However, this does not pose a problem\n",
    "for end-to-end Neural-Symbolic systems such as LTN, for which the\n",
    "gradients can propagate through the logical structures.\n",
    "\n",
    "We start by illustrating an LTN theory that can be used to learn the predicate\n",
    "$digit$. The specification of the theory below is for the single digit addition example,\n",
    "although it can be extended easily to the multiple digits case:\n",
    "\n",
    "**Domains:**\n",
    "- $images$, denoting the MNIST digit images;\n",
    "- $results$, denoting the integers which are the labels for the results of the additions;\n",
    "- $digits$, denoting the digits from 0 to 9.\n",
    "\n",
    "**Variables:**\n",
    "- $x, y$ ranging over the MNIST images;\n",
    "- $n$ for the labels, i.e., the results of the additions;\n",
    "- $d_1, d_2$ ranging over digits;\n",
    "- $D(x) = D(y) = images$;\n",
    "- $D(n) = results$;\n",
    "- $D(d_1) = d(d_2) = digits$.\n",
    "\n",
    "**Predicates:**\n",
    "- $digit(x, d)$ for the single digit classifier, where $d$ is a term denoting a digit\n",
    "constant or a digit variable. The classifier should return the probability of an\n",
    "image $x$ being of digit $d$;\n",
    "- $D_{in}(digit) = images,digits$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "Single digit addition:\n",
    "\n",
    "$\\forall Diag(x, y, n) \\text{ } (\\exists d_1, d_2 : d_1 + d_2 = n \\text{ } (digit(x, d_1) \\land digit(y, d_2)))$\n",
    "\n",
    "Multiple digit addition:\n",
    "\n",
    "$\\forall Diag(x_1, x_2, y_1, y_2, n) \\text{ } (\\exists d_1, d_2, d_3, d_4: 10d_1 + d_2 + 10d_3 + d_4 = n \\text{ } (digit(x_1, d_1) \\land digit(x_2, d_2) \\land digit(y_1, d_3) \\land digit(y_2, d_4)))$\n",
    "\n",
    "Notice the use of $Diag$: when grounding $x,y,n$ with three sequences of values,\n",
    "the *i-th* examples of each variable are matching. That is, $(\\mathcal{G}(x)_i, \\mathcal{G}(y)_i, \\mathcal{G}(n)_i)$ is\n",
    "a tuple from our dataset of valid additions. Using the diagonal quantification,\n",
    "LTN aggregates pairs of images and their corresponding result, rather than any\n",
    "combination of images and results.\n",
    "\n",
    "Notice also the guarded quantification: by quantifying only on the \"intermediate labels\" (not given during training)\n",
    "that could add up to the result label\n",
    "(given during training), we incorporate symbolic information into the system.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(images)=[0, 1]^{28 \\times 28 \\times 1}$. The MNIST dataset has images of 28 by 28 pixels.\n",
    "The images are grayscale and have just one channel. The RGB pixel values from\n",
    "0 to 255 of the MNIST dataset are scaled to the range [0, 1];\n",
    "- $\\mathcal{G}(results)=\\mathbb{N}$;\n",
    "- $\\mathcal{G}(digits) = {0, 1, \\dots, 9}$;\n",
    "- $\\mathcal{G}(x) \\in[0,1]^{m \\times 28 \\times 28 \\times 1}, \\mathcal{G}(y) \\in[0,1]^{m \\times 28 \\times 28 \\times 1}, \\mathcal{G}(n) \\in \\mathbb{N}^{m}$.\n",
    "Notice the use of the same number $m$ of examples for each of these variables as they are supposed\n",
    "to match one-to-one due to the use of $Diag$;\n",
    "- $\\mathcal{G}\\left(d_{1}\\right)=\\mathcal{G}\\left(d_{2}\\right)=\\langle 0,1, \\ldots, 9\\rangle$;\n",
    "- $\\mathcal{G}(digit \\mid \\theta): x, d \\rightarrow \\operatorname{onehot}(d)^{\\top} \\cdot \\operatorname{softmax}\\left(\\operatorname{CNN}_{\\theta}(x)\\right)$, where $CNN$\n",
    "is a Convolutional Neural Network with 10 output neurons for each class. Notice that, in contrast\n",
    "with the previous examples, $d$ is an integer label; $onehot(d)$ converts it into a\n",
    "one-hot label.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import and create the dataset.\n",
    "\n",
    "The MNIST dataset contains 70000 images, subdivided into 60000 examples for training and 10000 for test. For our task,\n",
    "we need to create an ad-hoc dataset starting from the MNIST dataset.\n",
    "\n",
    "For the single digit case, the first 30000 images of the training set are used as left operands for the addition, while\n",
    "the last 30000 images are used as right operands. The sum of the labels of the left and right operands is used as target.\n",
    "The same process is repeated to create the test set.\n",
    "\n",
    "For the multiple digit case, the training set is divided into four groups of 15000 images. The first two groups form the\n",
    "left operands for the additions, while the last two groups the right operands. The target is created in a similar manner\n",
    "to the previous case.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 半监督模式识别\n",
    "\n",
    "让我们现在探索两个更复杂的分类任务，以展示将逻辑推理与机器学习相结合的好处。\n",
    "\n",
    "任务是预测给定输入的两个数字的和。我们有这些数字的图像，但不知道它们的类别，即标签。我们唯一知道的是它们的和。特别是，第一个任务较简单，包含预测两个个位数的和。第二个任务更复杂，因为它涉及预测两个多位数的和。\n",
    "\n",
    "为了创建我们的数据集，我们使用了流行的MNIST数据集，其中包含标记的手写数字。\n",
    "\n",
    "**个位数相加：**\n",
    "考虑谓词 $addition(X, Y, n)$，其中 $X$ 和 $Y$ 是数字的图像，$n$ 是对应这些数字和的自然数。这个谓词应该返回对加法有效性的估计。例如，$addition(img(8), img(3), 11)$ 是一个有效的加法，而 $addition(img(3), img(3), 5)$ 则不是。在这个例子中，$img(x)$ 意味着“$x$ 的图像”，其中 $x$ 是一个数字标签（digit label）。这与逻辑函数（a logical function）不同。\n",
    "\n",
    "**多位数相加：**\n",
    "实验扩展到多位数。考虑谓词 $addition([img(X_1), img(X_2)],[img(Y_1), img(Y_2)], n)$，其中 $[img(X_1), img(X_2)]$ 和 $[img(Y_1), img(Y_2)]$ 是数字图像列表，代表两个多位数，$n$ 是这两个多位数和的自然数。例如，$addition([img(2), img(0)], [img(1), (7)], 37)$ 是一个有效的加法，而 $addition([img(5), img(4)], [img(9), img(0)], 50)$ 则不是。\n",
    "\n",
    "一种自然的神经符号方法是尝试学习一个个位数分类器，并利用关于加法性质的现有知识。例如，假设谓词 $digit(x,d)$ 给出了图像 $x$ 是数字 $d$ 的可能性。在 LTN 中 $addition(img(3), img(8), 11)$ 的定义为（addition这个谓词是理论上的，在具体的代码实现中，没有这个谓词，这个谓词定义为下面的式子）：（这里的d1+d2=11，是条件量化，计算存在的时候，只计算d1和d2相加为11的那些d1和d2。你从头分析，会发现这里是一个二维矩阵，d1和d2分别是两个轴）\n",
    "\n",
    "$$\n",
    "\\exists d_1, d_2 : d_1 + d_2 = 11 \\text{ } (digit(img(3), d_1) \\land digit(img(8), d_2))\n",
    "$$\n",
    "\n",
    "通过不提供训练期间的单个数字图像标签，上述任务变得更加复杂。相反，训练在一对图像上进行，只提供结果的标签，即个体标签的和。单个数字分类器不会单独显式训练。它的输出是逻辑使用的潜在信息。然而，这对像 LTN 这样的端到端神经符号系统并不构成问题，因为梯度可以通过逻辑结构传播。\n",
    "\n",
    "我们首先说明一个可以用于学习谓词 $digit$ 的 LTN 理论。下面的理论规格是针对单个数字加法示例的，但可以轻松扩展到多位数情况：\n",
    "\n",
    "**领域：**\n",
    "- $images$，表示 MNIST 数字图像；\n",
    "- $results$，表示加法结果的整数标签；\n",
    "- $digits$，表示 0 到 9 的数字。\n",
    "\n",
    "**变量：**\n",
    "- $x, y$ 取值于 MNIST 图像；\n",
    "- $n$ 表示标签，即加法结果；\n",
    "- $d_1, d_2$ 取值于数字；\n",
    "- $D(x) = D(y) = images$；\n",
    "- $D(n) = results$；\n",
    "- $D(d_1) = D(d_2) = digits$。\n",
    "\n",
    "**谓词：**\n",
    "- $digit(x, d)$ 用于单个数字分类器，其中 $d$ 是表示数字常量或数字变量的项。分类器应返回图像 $x$ 是数字 $d$ 的概率；\n",
    "- $D_{in}(digit) = images,digits$。这里的这个digit指的是前面的那个谓词digit\n",
    "\n",
    "**公理：**\n",
    "\n",
    "单个数字加法：\n",
    "\n",
    "$$\n",
    "\\forall Diag(x, y, n) \\text{ } (\\exists d_1, d_2 : d_1 + d_2 = n \\text{ } (digit(x, d_1) \\land digit(y, d_2)))\n",
    "$$\n",
    "\n",
    "多位数加法：\n",
    "\n",
    "$$\n",
    "\\forall Diag(x_1, x_2, y_1, y_2, n) \\text{ } (\\exists d_1, d_2, d_3, d_4: 10d_1 + d_2 + 10d_3 + d_4 = n \\text{ } (digit(x_1, d_1) \\land digit(x_2, d_2) \\land digit(y_1, d_3) \\land digit(y_2, d_4)))\n",
    "$$\n",
    "\n",
    "注意使用 $Diag$：当用三个值序列对 $x,y,n$ 进行基础化时，每个变量的第 *i* 个示例是匹配的。即 $(\\mathcal{G}(x)_i, \\mathcal{G}(y)_i, \\mathcal{G}(n)_i)$ 是我们有效加法数据集中的一个元组。使用对角量化，LTN 聚合成对图像及其对应的结果，而不是任意组合的图像和结果。\n",
    "\n",
    "还请注意守卫量化：仅对可能加起来等于结果标签（训练期间未提供）的中间标签进行量化，我们将符号信息引入系统。\n",
    "\n",
    "**基础化：**\n",
    "- $\\mathcal{G}(images)=[0, 1]^{28 \\times 28 \\times 1}$。MNIST 数据集的图像为 28 x 28 像素。图像是灰度的，只有一个通道。MNIST 数据集的 RGB 像素值从 0 到 255 缩放到 [0, 1] 范围；\n",
    "- $\\mathcal{G}(results)=\\mathbb{N}$；\n",
    "- $\\mathcal{G}(digits) = {0, 1, \\dots, 9}$；\n",
    "- $\\mathcal{G}(x) \\in[0,1]^{m \\times 28 \\times 28 \\times 1}, \\mathcal{G}(y) \\in[0,1]^{m \\times 28 \\times 28 \\times 1}, \\mathcal{G}(n) \\in \\mathbb{N}^{m}$。注意对每个变量使用相同数量的示例 $m$，因为它们应该由于使用 $Diag$ 而一一匹配；\n",
    "- $\\mathcal{G}\\left(d_{1}\\right)=\\mathcal{G}\\left(d_{2}\\right)=\\langle 0,1, \\ldots, 9\\rangle$；\n",
    "- $\\mathcal{G}(digit \\mid \\theta): x, d \\rightarrow \\operatorname{onehot}(d)^{\\top} \\cdot \\operatorname{softmax}\\left(\\operatorname{CNN}_{\\theta}(x)\\right)$，其中 $CNN$ 是一个具有每类 10 个输出神经元的卷积神经网络。注意，与前面的示例不同，$d$ 是一个整数标签；$onehot(d)$ 将其转换为一个独热标签。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "现在，让我们导入并创建数据集。\n",
    "\n",
    "MNIST 数据集包含 70000 张图像，分为 60000 个训练示例和 10000 个测试示例。对于我们的任务，我们需要从 MNIST 数据集创建一个特定的数据集。\n",
    "\n",
    "对于个位数情况，训练集的前 30000 张图像用作加法的左操作数，最后 30000 张图像用作右操作数。左操作数和右操作数标签的和用作目标。相同的过程用于创建测试集。\n",
    "\n",
    "对于多位数情况，训练集分为四组，每组 15000 张图像。前两组形成加法的左操作数，而后两组形成右操作数。目标的创建方式与前一种情况类似。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "\n",
    "def get_mnist_dataset_for_digits_addition(single_digit=True):\n",
    "    \"\"\"\n",
    "    It prepares the dataset for the MNIST single digit or multi digits addition example of the LTN paper.\n",
    "\n",
    "    :param single_digit: whether the dataset has to be generated for the single digit or multi digits example (please,carefully read the examples in the paper to understand the differences between the two).\n",
    "    :return: a tuple of two elements. The first element is the training set, while the second element is the test set.\n",
    "    Both training set and test set are lists that contain the following information:\n",
    "        1. a list [left_operands, right_operands], where left_operands is a list of MNIST images that are used as the\n",
    "        left operand of the addition, while right_operands is a list of MNIST images that are used as the right operand\n",
    "        of the addition;\n",
    "        2. a list containing the summation of the labels of the images contained in the list at point 1. The label of\n",
    "        the left operand is added to the label of the right operand, and the target label is generated. This represents\n",
    "        the target of the digits addition task.\n",
    "    Note that this is the output of the process for the single digit case. In the multi digits case the list at point\n",
    "    1 will have 4 elements since in the multi digits case four digits are involved in each addition (two digits\n",
    "    represent the first operand and two digits the second operand).\n",
    "    \n",
    "    它为 LTN 论文中的 MNIST 单个数字或多个数字加法示例准备数据集。\n",
    "    \n",
    "    :param single_digit: 数据集是否需要为单个数字或多个数字示例生成（请仔细阅读论文中的示例以了解两者之间的区别）。 # todo:详细阅读\n",
    "    :return: 一个包含两个元素的元组。第一个元素是训练集，第二个元素是测试集。训练集和测试集都是包含以下信息的列表：（意思应该是元组中包含两个列表。每个列表又包含两个列表）\n",
    "    1. 一个列表 [left_operands, right_operands]，其中 left_operands 是用于加法左操作数的 MNIST 图像列表，而 right_operands 是用于加法右操作数的 MNIST 图像列表；\n",
    "    2. 一个包含第 1 点中列表中图像标签和的列表。左操作数的标签与右操作数的标签相加，生成目标标签。这代表数字加法任务的目标。\n",
    "    请注意，这是单个数字情况的输出。在多个数字情况下，第 1 点中的列表将有 4 个元素，因为在多个数字情况下，每个加法涉及四个数字（两个数字代表第一个操作数，两个数字代表第二个操作数）。\n",
    "    \"\"\"\n",
    "    if single_digit:\n",
    "        n_train_examples = 30000\n",
    "        n_test_examples = 5000\n",
    "        n_operands = 2 # 设置操作数的数量，单个数字是2个操作数，多个数字是4个操作数。\n",
    "    else:\n",
    "        n_train_examples = 15000\n",
    "        n_test_examples = 2500\n",
    "        n_operands = 4\n",
    "\n",
    "    mnist_train = torchvision.datasets.MNIST(\"./datasets/\", train=True, download=True,transform=torchvision.transforms.ToTensor()) # 返回值类型是torchvision.datasets.mnist.MNIST # \n",
    "    mnist_test = torchvision.datasets.MNIST(\"./datasets/\", train=False, download=True,transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    train_imgs, train_labels, test_imgs, test_labels = mnist_train.data, mnist_train.targets, mnist_test.data, mnist_test.targets\n",
    "\n",
    "    train_imgs, test_imgs = train_imgs / 255.0, test_imgs / 255.0\n",
    "\n",
    "    train_imgs, test_imgs = torch.unsqueeze(train_imgs, 1), torch.unsqueeze(test_imgs, 1)\n",
    "\n",
    "    imgs_operand_train = [train_imgs[i * n_train_examples:i * n_train_examples + n_train_examples] for i in range(n_operands)]\n",
    "    labels_operand_train = [train_labels[i * n_train_examples:i * n_train_examples + n_train_examples] for i in range(n_operands)]\n",
    "\n",
    "    imgs_operand_test = [test_imgs[i * n_test_examples:i * n_test_examples + n_test_examples] for i in range(n_operands)]\n",
    "    labels_operand_test = [test_labels[i * n_test_examples:i * n_test_examples + n_test_examples] for i in range(n_operands)]\n",
    "\n",
    "    if single_digit:\n",
    "        label_addition_train = labels_operand_train[0] + labels_operand_train[1]\n",
    "        label_addition_test = labels_operand_test[0] + labels_operand_test[1]\n",
    "    else:\n",
    "        label_addition_train = 10 * labels_operand_train[0] + labels_operand_train[1] + 10 * labels_operand_train[2] + labels_operand_train[3]\n",
    "\n",
    "        label_addition_test = 10 * labels_operand_test[0] + labels_operand_test[1] + 10 * labels_operand_test[2] + labels_operand_test[3]\n",
    "\n",
    "    train_set = [torch.stack(imgs_operand_train, dim=1), label_addition_train]\n",
    "    test_set = [torch.stack(imgs_operand_test, dim=1), label_addition_test]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "# single digit dataset # 单个数字数据集\n",
    "single_d_train_set, single_d_test_set = get_mnist_dataset_for_digits_addition(single_digit=True)\n",
    "# multi digit dataset # 多个数字数据集\n",
    "multi_d_train_set, multi_d_test_set = get_mnist_dataset_for_digits_addition(single_digit=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:53:46.159530Z",
     "start_time": "2024-08-07T10:53:41.099948Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mircocrift\\AppData\\Local\\Temp\\ipykernel_25924\\1318613235.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just for illustration, we show the first example of the training set for the single digit case. This should help in\n",
    "understanding how the dataset is created."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "仅供说明，我们展示了单个数字案例训练集的第一个示例。这应该有助于理解数据集是如何创建的。"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "first_example_images = single_d_train_set[0][0]\n",
    "first_example_label = single_d_train_set[1][0]\n",
    "\n",
    "print(\"Operands are displayed in the following images:\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(first_example_images[0].permute(1, 2, 0))\n",
    "ax.set_title('First operand')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(first_example_images[1].permute(1, 2, 0))\n",
    "ax.set_title('Second operand')\n",
    "plt.show()\n",
    "print(\"The target label (sum) for these operands is: %d\" % first_example_label.item())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:54:45.400421Z",
     "start_time": "2024-08-07T10:54:44.682961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operands are displayed in the following images:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target label (sum) for these operands is: 8\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define predicate $digit$, variables $d_1$, $d_2$, $d_3$, $d_4$,\n",
    "connectives, universal and existential quantifiers, and the `SatAgg` operator.\n",
    "\n",
    "For connectives and quantifiers, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For predicate $digit$, we have two models. The first one implements a $CNN$ which outputs the logits for the ten classes of\n",
    "the MNIST dataset, given an image $x$ in input. The second model takes as input a labelled example $(x,d)$, it computes the logits\n",
    "using the first model and then returns the prediction (*softmax*) for class $d$. In other words, it computes the likelihood\n",
    "of image $d$ being of digit $d$.\n",
    "\n",
    "We need two separated models because we need both logits and probabilities. Logits are used to compute the classification\n",
    "accuracy, while probabilities are interpreted as truth values to compute the satisfaction level of the knowledge base.\n",
    "\n",
    "The variables $d_1$, $d_2$, $d_3$, and $d_4$ represent the 10 digit labels of the MNIST dataset, namely they are the sequence $\\langle 0,1, \\ldots, 9\\rangle$.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LTN 设置\n",
    "\n",
    "为了定义我们的知识库（公理），我们需要定义谓词 $digit$，变量 $d_1$，$d_2$，$d_3$，$d_4$，连接词，全称和存在量词，以及 `SatAgg` 操作符。\n",
    "\n",
    "对于连接词和量词，我们使用稳定乘积配置（在教程中已见过）。\n",
    "\n",
    "对于谓词 $digit$，我们有两个模型。第一个模型实现了一个 $CNN$，它根据输入图像 $x$ 输出 MNIST 数据集的十个类别的 logits。第二个模型以一个标记示例 $(x,d)$ 作为输入，使用第一个模型计算 logits，然后返回类别 $d$ 的预测值（*softmax*）。换句话说，它计算图像 $d$ 是数字 $d$ 的可能性。\n",
    "\n",
    "我们需要两个独立的模型，因为我们既需要 logits 也需要概率。logits 用于计算分类准确性，而概率被解释为真值，用于计算知识库的满足度。\n",
    "\n",
    "变量 $d_1$，$d_2$，$d_3$ 和 $d_4$ 表示 MNIST 数据集的 10 个数字标签，即它们是序列 $\\langle 0,1, \\ldots, 9\\rangle$。\n",
    "\n",
    "`SatAgg` 使用 `pMeanError` 聚合器定义。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn.init import xavier_uniform_, normal_, kaiming_uniform_\n",
    "import ltn\n",
    "\n",
    "# we define the variables # 定义变量\n",
    "d_1 = ltn.Variable(\"d_1\", torch.tensor(range(10)))\n",
    "d_2 = ltn.Variable(\"d_2\", torch.tensor(range(10)))\n",
    "# these are used only in the multi digit case # 这些仅在多个数字的情况下使用\n",
    "d_3 = ltn.Variable(\"d_3\", torch.tensor(range(10)))\n",
    "d_4 = ltn.Variable(\"d_4\", torch.tensor(range(10)))\n",
    "\n",
    "# we define predicate digit # 我们定义谓词digit\n",
    "class MNISTConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    CNN that returns linear embeddings for MNIST images. It is used in the single digit and multi digits addition\n",
    "    examples of the LTN paper.\n",
    "    Args:\n",
    "        conv_channels_sizes: tuple containing the number of channels of the convolutional layers of the model. The first\n",
    "        element of the tuple must be the number of input channels of the first conv layer, while the last element\n",
    "        of the tuple must be the number of output channels of the last conv layer. Specifically, the number of conv\n",
    "        layers constructed is equal to `len(conv_channels_sizes) - 1`;\n",
    "        kernel_sizes: tuple containing the sizes of the kernels used in the conv layers of the architecture;\n",
    "        linear_layers_sizes: tuple containing the sizes of the linear layers used as the final layers of the\n",
    "        architecture. The first element of the tuple must be the number of features in input to the first linear layer,\n",
    "        while the last element of the tuple must be the number of output features of the last linear layer. Specifically,\n",
    "        the number of layers constructed is equal to `len(linear_layers_sizes) - 1`.\n",
    "    用于 MNIST 图像的返回线性嵌入的 CNN。在 LTN 论文的单个数字和多个数字加法示例中使用。\n",
    "    \n",
    "    参数：\n",
    "    - `conv_channels_sizes`：包含模型卷积层通道数的元组。元组的第一个元素必须是第一个卷积层的输入通道数，而最后一个元素必须是最后一个卷积层的输出通道数。具体来说，卷积层的数量等于 `len(conv_channels_sizes) - 1`。\n",
    "    - `kernel_sizes`：包含架构中卷积层使用的核大小的元组。\n",
    "    - `linear_layers_sizes`：包含作为架构最终层使用的线性层大小的元组。元组的第一个元素必须是第一个线性层的输入特征数，而最后一个元素必须是最后一个线性层的输出特征数。具体来说，构建的层数等于 `len(linear_layers_sizes) - 1`。\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_channels_sizes=(1, 6, 16), kernel_sizes=(5, 5), linear_layers_sizes=(256, 100)):\n",
    "        super(MNISTConv, self).__init__()\n",
    "        self.conv_layers = torch.nn.ModuleList([torch.nn.Conv2d(conv_channels_sizes[i - 1], conv_channels_sizes[i],\n",
    "                                                                kernel_sizes[i - 1])\n",
    "                                                  for i in range(1, len(conv_channels_sizes))])\n",
    "        self.relu = torch.nn.ReLU()  # relu is used as activation for the conv layers # relu用作激活函数\n",
    "        self.tanh = torch.nn.Tanh()  # tanh is used as activation for the linear layers # tanh用作激活函数\n",
    "        self.maxpool = torch.nn.MaxPool2d((2, 2))\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(linear_layers_sizes[i - 1], linear_layers_sizes[i])\n",
    "                                                  for i in range(1, len(linear_layers_sizes))])\n",
    "        self.batch_norm_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(linear_layers_sizes[i])\n",
    "                                                      for i in range(1, len(linear_layers_sizes))])\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.relu(conv(x))\n",
    "            x = self.maxpool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            x = self.tanh(self.batch_norm_layers[i](self.linear_layers[i](x)))\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        r\"\"\"Initialize the weights of the network.\n",
    "        Weights of conv layers are initialized with the :py:func:`torch.nn.init.kaiming_uniform_` initializer,\n",
    "        weights of linear layers are initialized with the :py:func:`torch.nn.init.xavier_uniform_` initializer,\n",
    "        while biases are initialized with the :py:func:`torch.nn.init.normal_` initializer.\n",
    "        初始化网络的权重。\n",
    "        卷积层的权重使用 :py:func:`torch.nn.init.kaiming_uniform_` 初始化器初始化，线性层的权重使用 :py:func:`torch.nn.init.xavier_uniform_` 初始化器初始化，而偏置项使用 :py:func:`torch.nn.init.normal_` 初始化器初始化。\n",
    "        \"\"\"\n",
    "        for layer in self.conv_layers:\n",
    "            kaiming_uniform_(layer.weight)\n",
    "            normal_(layer.bias)\n",
    "\n",
    "        for layer in self.linear_layers:\n",
    "            xavier_uniform_(layer.weight)\n",
    "            normal_(layer.bias)\n",
    "\n",
    "\n",
    "class SingleDigitClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model classifying one MNIST digit image into 10 possible classes. It outputs the logits, so it is not a normalized\n",
    "    output. It has a convolutional part in the initial layers of the architecture and a linear part in the last\n",
    "    layers of the architecture. To build the convolutional part a pre-configured convolutional model is used.\n",
    "    Args:\n",
    "        layers_sizes: tuple containing the sizes of the linear layers used as the final layers of the\n",
    "        architecture. The first element of the tuple must be the number of features in input to the first linear layer,\n",
    "        while the last element of the tuple must be the number of output features of the last linear layer. Specifically,\n",
    "        the number of layers constructed is equal to `len(layers_sizes) - 1`.\n",
    "\n",
    "    用于将一个 MNIST 数字图像分类为 10 个可能类别的模型。它输出 logits，因此它不是一个归一化的输出。该模型在架构的初始层中有一个卷积部分，在架构的最后层中有一个线性部分。为了构建卷积部分，使用了一个预配置的卷积模型。\n",
    "        \n",
    "    参数：\n",
    "    - layers_sizes: 包含作为架构最终层的线性层大小的元组。元组的第一个元素必须是输入到第一个线性层的特征数量，而元组的最后一个元素必须是最后一个线性层的输出特征数量。具体来说，构建的层数等于 `len(layers_sizes) - 1`。\n",
    "    \"\"\"\n",
    "    def __init__(self, layers_sizes=(100, 84, 10)):\n",
    "        super(SingleDigitClassifier, self).__init__()\n",
    "        self.mnistconv = MNISTConv()  # this is the convolutional part of the architecture # 这是架构的卷积部分\n",
    "        self.tanh = torch.nn.Tanh()  # tanh is used as activation for the linear layers # tanh用作激活函数\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layers_sizes[i - 1], layers_sizes[i])\n",
    "                                                  for i in range(1, len(layers_sizes))])\n",
    "        self.batch_norm_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(layers_sizes[i])\n",
    "                                                      for i in range(1, len(layers_sizes))])\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mnistconv(x)\n",
    "        for i in range(len(self.linear_layers) - 1):\n",
    "            x = self.tanh(self.batch_norm_layers[i](self.linear_layers[i](x)))\n",
    "        return self.linear_layers[-1](x)  # in the last layer a sigmoid or a softmax has to be applied # 在最后一层应用sigmoid或softmax\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights of the linear layers of the network.\n",
    "        Weights are initialized with the :py:func:`torch.nn.init.xavier_uniform_` initializer,\n",
    "        while biases are initialized with the :py:func:`torch.nn.init.normal_` initializer.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers:\n",
    "            xavier_uniform_(layer.weight)\n",
    "            normal_(layer.bias)\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label d. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related to the given class d.\n",
    "    初始化网络的线性层权重。\n",
    "    权重使用 :py:func:`torch.nn.init.xavier_uniform_` 初始化器进行初始化，\n",
    "    而偏置使用 :py:func:`torch.nn.init.normal_` 初始化器进行初始化。\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        logits = self.logits_model(x)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.gather(probs, 1, d)\n",
    "        return out\n",
    "\n",
    "# single digit # 单个数字\n",
    "# cnn_s_d = SingleDigitClassifier() # 原来的写法会报错\n",
    "cnn_s_d = SingleDigitClassifier().to(ltn.device)\n",
    "Digit_s_d = ltn.Predicate(LogitsToPredicate(cnn_s_d)).to(ltn.device)\n",
    "# multi digit # 多个数字\n",
    "# cnn_m_d = SingleDigitClassifier() # 原来的写法会报错\n",
    "cnn_m_d = SingleDigitClassifier().to(ltn.device)\n",
    "Digit_m_d = ltn.Predicate(LogitsToPredicate(cnn_m_d)).to(ltn.device)\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg # 我们定义连接词、量词和SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:56:05.148188Z",
     "start_time": "2024-08-07T10:56:04.950190Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data. We create these loaders both for\n",
    "the single digit case and the multiple digit case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 工具\n",
    "\n",
    "现在，我们需要定义一些实用的类和函数。\n",
    "\n",
    "我们定义了一个标准的 PyTorch 数据加载器，它将数据集作为输入，并返回一个数据批次的生成器。特别地，我们需要一个用于训练数据的数据加载器实例和一个用于测试数据的数据加载器实例。我们为单个数字情况和多个数字情况创建这些加载器。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model # 这是一个标准的PyTorch DataLoader，用于加载模型的训练和测试数据集\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 fold,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.fold = fold\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.fold[0].shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.fold[0].shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            digits = self.fold[0][idxlist[start_idx:end_idx]]\n",
    "            addition_labels = self.fold[1][idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield digits, addition_labels\n",
    "\n",
    "# create train and test loader # 创建训练和测试加载器\n",
    "single_train_loader = DataLoader(single_d_train_set, 32, shuffle=True)\n",
    "single_test_loader = DataLoader(single_d_test_set, 32, shuffle=False)\n",
    "multi_d_train_loader = DataLoader(multi_d_train_set, 32, shuffle=True)\n",
    "multi_d_test_loader = DataLoader(multi_d_test_set, 32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:56:11.270582Z",
     "start_time": "2024-08-07T10:56:11.262713Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "### Single digit\n",
    "\n",
    "In the following, we learn our LTN for the single digit case. We train our model for 20 epochs and use the `Adam`\n",
    "optimizer with a batch size of 32.\n",
    "\n",
    "Note that hyper-parameter $p$ for existential quantification follows a schedule, changing from $p=1$ to $p=6$ gradually\n",
    "with the number of training epochs. If you are interested in the motivation behind this choice, consider\n",
    "reading the paper.\n",
    "\n",
    "The accuracy is measured by predicting the digit values using the predicate $digit$ and reporting the ratio of\n",
    "examples for which the addition is correct. Within each epoch, satisfaction level and accuracy on both training and\n",
    "test data are visualized.\n",
    "\n",
    "The following figure shows the LTN computational graph for the single digit case.\n",
    "\n",
    "![Computational graph](/examples/images/semisupervised-single.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 学习\n",
    "\n",
    "让我们定义 $D$ 为所有示例的数据集。目标函数由 $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$ 给出。\n",
    "\n",
    "在实践中，优化器使用以下损失函数：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)\n",
    "$$\n",
    "\n",
    "其中，$B$ 是从 $D$ 中采样的一个小批量。\n",
    "\n",
    "### 单一数字\n",
    "\n",
    "接下来，我们为单一数字案例学习我们的 LTN。我们将模型训练 20 个周期，并使用批大小为 32 的 `Adam` 优化器。\n",
    "\n",
    "请注意，存在量化的超参数 $p$ 遵循一个进度表，随着训练周期的增加，逐渐从 $p=1$ 变为 $p=6$。如果你对这种选择背后的动机感兴趣，可以阅读论文。\n",
    "\n",
    "通过使用谓词 $digit$ 预测数字值，并报告加法正确的示例比例来测量准确率。在每个周期内，训练和测试数据的满意度水平和准确率都会被可视化。\n",
    "\n",
    "下图显示了单一数字案例的 LTN 计算图。\n",
    "\n",
    "![计算图](/examples/images/semisupervised-single.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2024/08/2024_08_07__18_57_48_9353f2.png)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(Digit_s_d.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # scheduling of the parameter p for the existential quantifier as described in the LTN paper # 根据LTN论文中描述的存在量词的参数p的调度\n",
    "    if epoch in range(0, 4):\n",
    "        p = 1\n",
    "    if epoch in range(4, 8):\n",
    "        p = 2\n",
    "    if epoch in range(8, 12):\n",
    "        p = 4\n",
    "    if epoch in range(12, 20):\n",
    "        p = 6\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "    train_sat, test_sat = 0.0, 0.0\n",
    "    train_acc, test_acc = 0.0, 0.0\n",
    "    # train step # 训练步骤\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(single_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # ground variables with current batch data # 使用当前批次数据对变量进行地面化\n",
    "        images_x = ltn.Variable(\"x\", operand_images[:, 0])\n",
    "        images_y = ltn.Variable(\"y\", operand_images[:, 1])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x, images_y, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2],\n",
    "                And(Digit_s_d(images_x, d_1), Digit_s_d(images_y, d_2)),\n",
    "                cond_vars=[d_1, d_2, labels_z],\n",
    "                cond_fn=lambda d1, d2, z: torch.eq(d1.value + d2.value, z.value),\n",
    "                p=p\n",
    "            )).value\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_sat += sat_agg\n",
    "        # compute train accuracy # 计算训练准确性\n",
    "        predictions_x = torch.argmax(cnn_s_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_y = torch.argmax(cnn_s_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions = predictions_x + predictions_y\n",
    "        train_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    train_loss = train_loss / len(single_train_loader)\n",
    "    train_sat = train_sat / len(single_train_loader)\n",
    "    train_acc = train_acc / len(single_train_loader)\n",
    "\n",
    "    # test step # 测试步骤\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(single_test_loader):\n",
    "        # ground variables with current batch data # 使用当前批次数据对变量进行地面化\n",
    "        images_x = ltn.Variable(\"x\", operand_images[:, 0])\n",
    "        images_y = ltn.Variable(\"y\", operand_images[:, 1])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x, images_y, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2],\n",
    "                And(Digit_s_d(images_x, d_1), Digit_s_d(images_y, d_2)),\n",
    "                cond_vars=[d_1, d_2, labels_z],\n",
    "                cond_fn=lambda d1, d2, z: torch.eq(d1.value + d2.value, z.value),\n",
    "                p=p\n",
    "            )).value\n",
    "        loss = 1. - sat_agg\n",
    "        test_loss += loss.item()\n",
    "        test_sat += sat_agg\n",
    "        # compute test accuracy # 计算测试准确性\n",
    "        predictions_x = torch.argmax(cnn_s_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_y = torch.argmax(cnn_s_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions = predictions_x + predictions_y\n",
    "        test_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    test_loss = test_loss / len(single_test_loader)\n",
    "    test_sat = test_sat / len(single_test_loader)\n",
    "    test_acc = test_acc / len(single_test_loader)\n",
    "\n",
    "    # we print metrics every epoch of training # 我们在每个训练周期中打印指标\n",
    "\n",
    "    print(\" epoch %d | Train loss %.4f | Train Sat %.4f | Train Acc %.4f | Test loss %.4f | Test Sat %.4f |\"\n",
    "                \" Test Acc %.4f \" %\n",
    "          (epoch, train_loss, train_sat, train_acc, test_loss, test_sat, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T11:09:54.982893Z",
     "start_time": "2024-08-07T10:58:17.292434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | Train loss 0.8566 | Train Sat 0.1434 | Train Acc 0.8501 | Test loss 0.8344 | Test Sat 0.1656 | Test Acc 0.9359 \n",
      " epoch 1 | Train loss 0.8345 | Train Sat 0.1655 | Train Acc 0.9480 | Test loss 0.8303 | Test Sat 0.1697 | Test Acc 0.9504 \n",
      " epoch 2 | Train loss 0.8312 | Train Sat 0.1688 | Train Acc 0.9605 | Test loss 0.8306 | Test Sat 0.1694 | Test Acc 0.9502 \n",
      " epoch 3 | Train loss 0.8299 | Train Sat 0.1701 | Train Acc 0.9647 | Test loss 0.8269 | Test Sat 0.1731 | Test Acc 0.9688 \n",
      " epoch 4 | Train loss 0.6161 | Train Sat 0.3839 | Train Acc 0.9686 | Test loss 0.6121 | Test Sat 0.3879 | Test Acc 0.9648 \n",
      " epoch 5 | Train loss 0.6116 | Train Sat 0.3884 | Train Acc 0.9747 | Test loss 0.6100 | Test Sat 0.3900 | Test Acc 0.9684 \n",
      " epoch 6 | Train loss 0.6094 | Train Sat 0.3906 | Train Acc 0.9773 | Test loss 0.6125 | Test Sat 0.3875 | Test Acc 0.9630 \n",
      " epoch 7 | Train loss 0.6087 | Train Sat 0.3913 | Train Acc 0.9785 | Test loss 0.6102 | Test Sat 0.3898 | Test Acc 0.9672 \n",
      " epoch 8 | Train loss 0.4002 | Train Sat 0.5998 | Train Acc 0.9770 | Test loss 0.4033 | Test Sat 0.5967 | Test Acc 0.9664 \n",
      " epoch 9 | Train loss 0.3947 | Train Sat 0.6053 | Train Acc 0.9809 | Test loss 0.4004 | Test Sat 0.5996 | Test Acc 0.9686 \n",
      " epoch 10 | Train loss 0.3931 | Train Sat 0.6069 | Train Acc 0.9820 | Test loss 0.3990 | Test Sat 0.6010 | Test Acc 0.9697 \n",
      " epoch 11 | Train loss 0.3912 | Train Sat 0.6088 | Train Acc 0.9832 | Test loss 0.3986 | Test Sat 0.6014 | Test Acc 0.9709 \n",
      " epoch 12 | Train loss 0.2980 | Train Sat 0.7020 | Train Acc 0.9833 | Test loss 0.3046 | Test Sat 0.6954 | Test Acc 0.9735 \n",
      " epoch 13 | Train loss 0.2950 | Train Sat 0.7050 | Train Acc 0.9853 | Test loss 0.3061 | Test Sat 0.6939 | Test Acc 0.9717 \n",
      " epoch 14 | Train loss 0.2927 | Train Sat 0.7073 | Train Acc 0.9860 | Test loss 0.3026 | Test Sat 0.6974 | Test Acc 0.9751 \n",
      " epoch 15 | Train loss 0.2906 | Train Sat 0.7094 | Train Acc 0.9873 | Test loss 0.3020 | Test Sat 0.6980 | Test Acc 0.9751 \n",
      " epoch 16 | Train loss 0.2903 | Train Sat 0.7097 | Train Acc 0.9872 | Test loss 0.3031 | Test Sat 0.6969 | Test Acc 0.9737 \n",
      " epoch 17 | Train loss 0.2896 | Train Sat 0.7104 | Train Acc 0.9876 | Test loss 0.3054 | Test Sat 0.6946 | Test Acc 0.9717 \n",
      " epoch 18 | Train loss 0.2886 | Train Sat 0.7114 | Train Acc 0.9887 | Test loss 0.3019 | Test Sat 0.6981 | Test Acc 0.9749 \n",
      " epoch 19 | Train loss 0.2875 | Train Sat 0.7125 | Train Acc 0.9888 | Test loss 0.3009 | Test Sat 0.6991 | Test Acc 0.9751 \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that variables $x$, $y$, and $n$ are grounded batch by batch with new data arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg`, differently from previous examples, is specified by one single axiom.\n",
    "\n",
    "This example shows the power of integrating neural networks with logical reasoning.\n",
    "\n",
    "### Multi digit\n",
    "\n",
    "The multi digit case is equal to the single digit case. The only difference is on how the variables are grounded to take\n",
    "into account more digits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "注意，变量 $x$、$y$ 和 $n$ 是逐批由数据加载器提供的新数据来进行基础化的。这正是我们所说的 $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$，其中 $B$ 是数据加载器采样的一个小批次。\n",
    "\n",
    "还要注意的是，`SatAgg` 与之前的示例不同，它是由一个单一的公理指定的。\n",
    "\n",
    "这个例子展示了将神经网络与逻辑推理相结合的强大功能。\n",
    "\n",
    "### 多位数字\n",
    "\n",
    "多位数字的情况与单个位数字的情况相同。唯一的区别是变量如何基础化以考虑更多的数字。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(Digit_m_d.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # scheduling of the parameter p for the existential quantifier as described in the LTN paper # 根据LTN论文中描述的存在量词的参数p的调度\n",
    "    if epoch in range(0, 4):\n",
    "        p = 1\n",
    "    if epoch in range(4, 8):\n",
    "        p = 2\n",
    "    if epoch in range(8, 12):\n",
    "        p = 4\n",
    "    if epoch in range(12, 20):\n",
    "        p = 6\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "    train_sat, test_sat = 0.0, 0.0\n",
    "    train_acc, test_acc = 0.0, 0.0\n",
    "    # train step # 训练步骤\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(multi_d_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # ground variables with current batch data # 使用当前批次数据对变量进行地面化\n",
    "        images_x1 = ltn.Variable(\"x1\", operand_images[:, 0])\n",
    "        images_x2 = ltn.Variable(\"x2\", operand_images[:, 1])\n",
    "        images_y1 = ltn.Variable(\"y1\", operand_images[:, 2])\n",
    "        images_y2 = ltn.Variable(\"y2\", operand_images[:, 3])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x1, images_x2, images_y1, images_y2, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2, d_3, d_4],\n",
    "                And(\n",
    "                    And(Digit_m_d(images_x1, d_1),Digit_m_d(images_x2, d_2)),\n",
    "                    And(Digit_m_d(images_y1, d_3),Digit_m_d(images_y2, d_4))\n",
    "                ),\n",
    "                cond_vars=[d_1, d_2, d_3, d_4, labels_z],\n",
    "                cond_fn=lambda d1, d2, d3, d4, z: torch.eq(10 * d1.value + d2.value + 10 * d3.value + d4.value, z.value),\n",
    "                p=p\n",
    "            )).value\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_sat += sat_agg\n",
    "        # compute train accuracy # 计算训练准确性\n",
    "        predictions_x1 = torch.argmax(cnn_m_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_x2 = torch.argmax(cnn_m_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions_y1 = torch.argmax(cnn_m_d(operand_images[:, 2].to(ltn.device)), dim=1)\n",
    "        predictions_y2 = torch.argmax(cnn_m_d(operand_images[:, 3].to(ltn.device)), dim=1)\n",
    "        predictions = 10 * predictions_x1 + predictions_x2 + 10 * predictions_y1 + predictions_y2\n",
    "        train_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    train_loss = train_loss / len(multi_d_train_loader)\n",
    "    train_sat = train_sat / len(multi_d_train_loader)\n",
    "    train_acc = train_acc / len(multi_d_train_loader)\n",
    "\n",
    "    # test step # 测试步骤\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(multi_d_test_loader):\n",
    "        # ground variables with current batch data # 使用当前批次数据对变量进行地面化\n",
    "        images_x1 = ltn.Variable(\"x1\", operand_images[:, 0])\n",
    "        images_x2 = ltn.Variable(\"x2\", operand_images[:, 1])\n",
    "        images_y1 = ltn.Variable(\"y1\", operand_images[:, 2])\n",
    "        images_y2 = ltn.Variable(\"y2\", operand_images[:, 3])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x1, images_x2, images_y1, images_y2, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2, d_3, d_4],\n",
    "                And(\n",
    "                    And(Digit_m_d(images_x1, d_1),Digit_m_d(images_x2, d_2)),\n",
    "                    And(Digit_m_d(images_y1, d_3),Digit_m_d(images_y2, d_4))\n",
    "                ),\n",
    "                cond_vars=[d_1, d_2, d_3, d_4, labels_z],\n",
    "                cond_fn=lambda d1, d2, d3, d4, z: torch.eq(10 * d1.value + d2.value + 10 * d3.value + d4.value, z.value),\n",
    "                p=p\n",
    "            )).value\n",
    "        loss = 1. - sat_agg\n",
    "        test_loss += loss.item()\n",
    "        test_sat += sat_agg\n",
    "        # compute test accuracy # 计算测试准确性\n",
    "        predictions_x1 = torch.argmax(cnn_m_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_x2 = torch.argmax(cnn_m_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions_y1 = torch.argmax(cnn_m_d(operand_images[:, 2].to(ltn.device)), dim=1)\n",
    "        predictions_y2 = torch.argmax(cnn_m_d(operand_images[:, 3].to(ltn.device)), dim=1)\n",
    "        predictions = 10 * predictions_x1 + predictions_x2 + 10 * predictions_y1 + predictions_y2\n",
    "        test_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    test_loss = test_loss / len(multi_d_test_loader)\n",
    "    test_sat = test_sat / len(multi_d_test_loader)\n",
    "    test_acc = test_acc / len(multi_d_test_loader)\n",
    "\n",
    "    # we print metrics every epoch of training # 我们在每个训练周期中打印指标\n",
    "\n",
    "    print(\" epoch %d | Train loss %.4f | Train Sat %.4f | Train Acc %.4f | Test loss %.4f | Test Sat %.4f |\"\n",
    "                \" Test Acc %.4f \" %\n",
    "          (epoch, train_loss, train_sat, train_acc, test_loss, test_sat, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T11:27:05.551670Z",
     "start_time": "2024-08-07T11:11:01.580727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | Train loss 0.9876 | Train Sat 0.0124 | Train Acc 0.6989 | Test loss 0.9832 | Test Sat 0.0168 | Test Acc 0.8426 \n",
      " epoch 1 | Train loss 0.9832 | Train Sat 0.0168 | Train Acc 0.8816 | Test loss 0.9820 | Test Sat 0.0180 | Test Acc 0.8979 \n",
      " epoch 2 | Train loss 0.9824 | Train Sat 0.0176 | Train Acc 0.9121 | Test loss 0.9816 | Test Sat 0.0184 | Test Acc 0.9082 \n",
      " epoch 3 | Train loss 0.9821 | Train Sat 0.0179 | Train Acc 0.9238 | Test loss 0.9813 | Test Sat 0.0187 | Test Acc 0.9193 \n",
      " epoch 4 | Train loss 0.8821 | Train Sat 0.1179 | Train Acc 0.9239 | Test loss 0.8793 | Test Sat 0.1207 | Test Acc 0.9177 \n",
      " epoch 5 | Train loss 0.8779 | Train Sat 0.1221 | Train Acc 0.9423 | Test loss 0.8777 | Test Sat 0.1223 | Test Acc 0.9300 \n",
      " epoch 6 | Train loss 0.8764 | Train Sat 0.1236 | Train Acc 0.9512 | Test loss 0.8768 | Test Sat 0.1232 | Test Acc 0.9379 \n",
      " epoch 7 | Train loss 0.8752 | Train Sat 0.1248 | Train Acc 0.9569 | Test loss 0.8765 | Test Sat 0.1235 | Test Acc 0.9371 \n",
      " epoch 8 | Train loss 0.6704 | Train Sat 0.3296 | Train Acc 0.9492 | Test loss 0.6690 | Test Sat 0.3310 | Test Acc 0.9335 \n",
      " epoch 9 | Train loss 0.6629 | Train Sat 0.3371 | Train Acc 0.9609 | Test loss 0.6671 | Test Sat 0.3329 | Test Acc 0.9403 \n",
      " epoch 10 | Train loss 0.6609 | Train Sat 0.3391 | Train Acc 0.9636 | Test loss 0.6655 | Test Sat 0.3345 | Test Acc 0.9434 \n",
      " epoch 11 | Train loss 0.6595 | Train Sat 0.3405 | Train Acc 0.9680 | Test loss 0.6668 | Test Sat 0.3332 | Test Acc 0.9415 \n",
      " epoch 12 | Train loss 0.5270 | Train Sat 0.4730 | Train Acc 0.9670 | Test loss 0.5363 | Test Sat 0.4637 | Test Acc 0.9415 \n",
      " epoch 13 | Train loss 0.5233 | Train Sat 0.4767 | Train Acc 0.9711 | Test loss 0.5316 | Test Sat 0.4684 | Test Acc 0.9506 \n",
      " epoch 14 | Train loss 0.5199 | Train Sat 0.4801 | Train Acc 0.9742 | Test loss 0.5307 | Test Sat 0.4693 | Test Acc 0.9490 \n",
      " epoch 15 | Train loss 0.5198 | Train Sat 0.4802 | Train Acc 0.9751 | Test loss 0.5303 | Test Sat 0.4697 | Test Acc 0.9513 \n",
      " epoch 16 | Train loss 0.5171 | Train Sat 0.4829 | Train Acc 0.9777 | Test loss 0.5285 | Test Sat 0.4715 | Test Acc 0.9537 \n",
      " epoch 17 | Train loss 0.5147 | Train Sat 0.4853 | Train Acc 0.9795 | Test loss 0.5319 | Test Sat 0.4681 | Test Acc 0.9482 \n",
      " epoch 18 | Train loss 0.5150 | Train Sat 0.4850 | Train Acc 0.9807 | Test loss 0.5323 | Test Sat 0.4677 | Test Acc 0.9478 \n",
      " epoch 19 | Train loss 0.5158 | Train Sat 0.4842 | Train Acc 0.9800 | Test loss 0.5341 | Test Sat 0.4659 | Test Acc 0.9470 \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "Notice that the $\\land$ operator is a binary operator. It is for this reason that we had to create a nested logical conjunction.",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "注意，$\\land$ 运算符是一个二元运算符。正因为如此，我们必须创建一个嵌套的逻辑合取。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
