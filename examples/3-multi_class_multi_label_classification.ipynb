{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class multi-label classification\n",
    "\n",
    "We now turn to multi-label classification, whereby multiple labels can be assigned to each example. As a first example\n",
    "of the reach of LTNs, we shall see how the previous example can be extended naturally using LTN to account for multiple\n",
    "labels, which is not always a trivial extension for most ML algorithms.\n",
    "\n",
    "The standard approach to the multi-label problem is to provide explicit negative examples for each class. By contrast,\n",
    "LTN can use background knowledge to relate classes directly to each other, thus becoming a powerful tool in the case of\n",
    "the multi-label problem, where typically the labelled data is scarce.\n",
    "\n",
    "We explore the Leptograpsus crabs dataset, consisting of 200 examples of 5 morphological measurements of 50 crabs.\n",
    "The task is to classify the crabs according to their colour and sex. There are four labels: blue, orange, male, and female.\n",
    "The colour labels are mutually-exclusive, and so are the labels for sex. LTN will be used to specify such information\n",
    "logically.\n",
    "\n",
    "For this specific task, LTN uses the following language and grounding:\n",
    "\n",
    "**Domains:**\n",
    "- $items$, denoting the examples from the crabs data set;\n",
    "- $labels$, denoting the class labels.\n",
    "\n",
    "**Variables:**\n",
    "- $x_{blue}, x_{orange}, x_{male}, x_{female}$ for the positive examples of each class;\n",
    "- $x$, used to denote all the examples;\n",
    "- $D(x_{blue}) = D(x_{orange}) = D(x_{male}) = D(x_{female}) = D(x) = items$.\n",
    "\n",
    "**Constants:**\n",
    "- $l_{blue}, l_{orange}, l_{male}, l_{female}$: the labels of each class;\n",
    "- $D(l_{blue}) = D(l_{orange}) = D(l_{male}) = D(l_{female}) = labels$.\n",
    "\n",
    "**Predicates:**\n",
    "- $P(x,l)$ denoting the fact that item $x$ is labelled as $l$;\n",
    "- $D_{in}(P) = items,labels$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "- $\\forall x_{blue} P(x_{blue}, l_{blue})$: all the examples coloured by blue should have label $l_{blue}$;\n",
    "- $\\forall x_{orange} P(x_{orange}, l_{orange})$: all the examples coloured by orange should have label $l_{orange}$;\n",
    "- $\\forall x_{male} P(x_{male}, l_{male})$: all the examples that are males should have label $l_{male}$;\n",
    "- $\\forall x_{female} P(x_{female}, l_{female})$: all the examples that are females should have label $l_{female}$;\n",
    "- $\\forall x \\lnot (P(x, l_{blue}) \\land P(x, l_{orange}))$: if an example $x$ is labelled as blue, it cannot be labelled\n",
    "as orange too;\n",
    "- $\\forall x \\lnot (P(x, l_{male}) \\land P(x, l_{female}))$: if an example $x$ is labelled as male, it cannot be labelled\n",
    "as female too.\n",
    "\n",
    "Notice how the last two logical rules represent the mutual exclusion of the labels on colour and sex, respectively.\n",
    "As a result, negative examples are not used explicitly in this specification.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(items)=\\mathbb{R}^{5}$, items are described by 5 features;\n",
    "- $\\mathcal{G}(labels)=\\mathbb{N}^{4}$, we use a one-hot encoding to represent labels;\n",
    "- $\\mathcal{G}(x_{blue}) \\in \\mathbb{R}^{m_1 \\times 5}, \\mathcal{G}(x_{orange}) \\in \\mathbb{R}^{m_2 \\times 5},\\mathcal{G}(x_{male}) \\in \\mathbb{R}^{m_3 \\times 5},\\mathcal{G}(x_{female}) \\in \\mathbb{R}^{m_4 \\times 5}$.\n",
    "These sequences are not mutually-exclusive, one example can for instance be in both $x_{blue}$ and $x_{male}$;\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{m \\times 5}$, that is, $\\mathcal{G}(x)$ is a sequence of all the examples;\n",
    "- $\\mathcal{G}(l_{blue}) = [1, 0, 0, 0]$, $\\mathcal{G}(l_{orange}) = [0, 1, 0, 0]$, $\\mathcal{G}(l_{male}) = [0, 0, 1, 0]$, $\\mathcal{G}(l_{female}) = [0, 0, 0, 1]$;\n",
    "- $\\mathcal{G}(P \\mid \\theta): x,l \\mapsto l^\\top \\cdot \\sigma\\left(\\operatorname{MLP}_{\\theta}(x)\\right)$, where $MLP$\n",
    "has four output neurons corresponding to as many labels, and $\\cdot$ denotes the dot product as a way of selecting an\n",
    "output for $\\mathcal{G}(P \\mid \\theta)$. In fact, multiplying the $MLP$’s output by the one-hot vector $l^\\top$ gives the probability\n",
    "corresponding to the label denoted by $l$. By contrast with the previous example, notice the use of a *sigmoid* function instead of a *softmax* function. We need that because labels are not mutually exclusive anymore.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import the dataset.\n",
    "\n",
    "The Leptograpsus crabs dataset consists of 200 examples. Every example is represented by 5 features. The dataset\n",
    "is subdivided into train and test set. In particular, we use 160 examples for training and 40 for test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 多类别多标签分类\n",
    "\n",
    "我们现在转向多标签分类，其中每个示例可以分配多个标签。作为LTN（逻辑张量网络）应用的第一个例子，我们将看到如何自然地使用LTN扩展之前的示例以处理多个标签，这对于大多数机器学习算法来说并不是一个简单的扩展。\n",
    "\n",
    "解决多标签问题的标准方法是为每个类别提供明确的负样本。相比之下，LTN可以**使用背景知识直接将类别彼此关联**，从而在**标签数据稀少**的情况下成为一种强大的工具。\n",
    "\n",
    "我们将探索Leptograpsus蟹数据集，该数据集包含50只蟹的5个形态测量值的200个示例。任务是根据蟹的颜色和性别对其进行分类。有四个标签：蓝色、橙色、雄性和雌性。颜色标签是互斥的，性别标签也是如此。LTN将用于逻辑上指定这些信息。\n",
    "\n",
    "对于这个特定任务，LTN使用以下语言和基础：\n",
    "\n",
    "**域：**\n",
    "- $items$，表示蟹数据集中的示例；\n",
    "- $labels$，表示类别标签。\n",
    "\n",
    "**变量：**\n",
    "- $x_{blue}, x_{orange}, x_{male}, x_{female}$ 表示每个类别的正例；\n",
    "- $x$，用于表示所有示例；\n",
    "- $D(x_{blue}) = D(x_{orange}) = D(x_{male}) = D(x_{female}) = D(x) = items$。\n",
    "\n",
    "**常量：**\n",
    "- $l_{blue}, l_{orange}, l_{male}, l_{female}$：每个类别的标签；\n",
    "- $D(l_{blue}) = D(l_{orange}) = D(l_{male}) = D(l_{female}) = labels$。\n",
    "\n",
    "**谓词：**\n",
    "- $P(x,l)$ 表示项 $x$ 被标记为 $l$；\n",
    "- $D_{in}(P) = items,labels$。\n",
    "\n",
    "**公理：**\n",
    "\n",
    "- $\\forall x_{blue} P(x_{blue}, l_{blue})$：所有标记为蓝色的示例应具有标签 $l_{blue}$；\n",
    "- $\\forall x_{orange} P(x_{orange}, l_{orange})$：所有标记为橙色的示例应具有标签 $l_{orange}$；\n",
    "- $\\forall x_{male} P(x_{male}, l_{male})$：所有雄性示例应具有标签 $l_{male}$；\n",
    "- $\\forall x_{female} P(x_{female}, l_{female})$：所有雌性示例应具有标签 $l_{female}$；\n",
    "- $\\forall x \\lnot (P(x, l_{blue}) \\land P(x, l_{orange}))$：如果示例 $x$ 被标记为蓝色，则不能被标记为橙色；\n",
    "- $\\forall x \\lnot (P(x, l_{male}) \\land P(x, l_{female}))$：如果示例 $x$ 被标记为雄性，则不能被标记为雌性。\n",
    "\n",
    "注意最后两条逻辑规则分别表示颜色和性别标签的互斥性。因此，在这个规范中没有显式使用负样本。\n",
    "\n",
    "**基础：**\n",
    "- $\\mathcal{G}(items)=\\mathbb{R}^{5}$，项由5个特征描述；\n",
    "- $\\mathcal{G}(labels)=\\mathbb{N}^{4}$，我们使用独热编码来表示标签；\n",
    "- $\\mathcal{G}(x_{blue}) \\in \\mathbb{R}^{m_1 \\times 5}, \\mathcal{G}(x_{orange}) \\in \\mathbb{R}^{m_2 \\times 5},\\mathcal{G}(x_{male}) \\in \\mathbb{R}^{m_3 \\times 5},\\mathcal{G}(x_{female}) \\in \\mathbb{R}^{m_4 \\times 5}$。这些序列不是互斥的，例如，一个示例可以同时在 $x_{blue}$ 和 $x_{male}$ 中；\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{m \\times 5}$，即 $\\mathcal{G}(x)$ 是所有示例的序列；\n",
    "- $\\mathcal{G}(l_{blue}) = [1, 0, 0, 0]$，$\\mathcal{G}(l_{orange}) = [0, 1, 0, 0]$，$\\mathcal{G}(l_{male}) = [0, 0, 1, 0]$，$\\mathcal{G}(l_{female}) = [0, 0, 0, 1]$；\n",
    "- $\\mathcal{G}(P \\mid \\theta): x,l \\mapsto l^\\top \\cdot \\sigma\\left(\\operatorname{MLP}_{\\theta}(x)\\right)$，其中 $MLP$ 有四个输出神经元，对应于多个标签，$\\cdot$ 表示点积作为选择 $\\mathcal{G}(P \\mid \\theta)$ 的输出的方法。实际上，将 $MLP$ 的输出与独热向量 $l^\\top$ 相乘会得到对应于标签 $l$ 的概率。与前一个例子相比，注意这里使用了 *sigmoid* 函数而不是 *softmax* 函数。我们需要这样做，因为标签不再是互斥的。\n",
    "\n",
    "### 数据集\n",
    "\n",
    "现在，让我们导入数据集。\n",
    "\n",
    "Leptograpsus蟹数据集包含200个示例。每个示例由5个特征表示。数据集被分为训练集和测试集。特别是，我们使用160个示例进行训练，40个示例进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/crabs.dat\", sep=\" \", skipinitialspace=True) # sep=\" \"表示以空格分隔，skipinitialspace=True表示跳过初始空格 # 返回值类型是DataFrame # skipinitialspace：是否跳过字段值前面的空格。\n",
    "df = df.sample(frac=1)  # shuffle dataset # 打乱数据集 # 返回值类型是DataFrame # frac：抽样比例，1 表示打乱所有数据。\n",
    "df = df.replace({'B': 0, 'O': 1, 'M': 2, 'F': 3})\n",
    "\n",
    "features = torch.tensor(df[['FL', 'RW', 'CL', 'CW', 'BD']].to_numpy())\n",
    "labels_sex = torch.tensor(df['sex'].to_numpy())\n",
    "labels_color = torch.tensor(df['sp'].to_numpy())\n",
    "\n",
    "train_data = features[:160].float()\n",
    "test_data = features[160:].float()\n",
    "train_sex_labels = labels_sex[:160].long()\n",
    "test_sex_labels = labels_sex[160:].long()\n",
    "train_color_labels = labels_color[:160].long()\n",
    "test_color_labels = labels_color[160:].long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:16:39.725389Z",
     "start_time": "2024-08-07T10:16:36.598141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mircocrift\\AppData\\Local\\Temp\\ipykernel_21836\\1548158648.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\mircocrift\\AppData\\Local\\Temp\\ipykernel_21836\\1548158648.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({'B': 0, 'O': 1, 'M': 2, 'F': 3})\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define predicate $P$, constants $l_{blue}$, $l_{orange}$, $l_{male}$,\n",
    "$l_{female}$, connectives, universal quantifier, and the `SatAgg` operator.\n",
    "\n",
    "For the connectives and quantifier, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For predicate $P$, we have two models. The first one implements an $MLP$ which outputs the logits for the four classes of\n",
    "the dataset, given an example $x$ in input. The second model takes as input a labelled example $(x,l)$, it computes the logits\n",
    "using the first model and then returns the prediction (*sigmoid*) for class $l$.\n",
    "\n",
    "The constants $l_{blue}$, $l_{orange}$, $l_{male}$, and $l_{female}$, represent the one-hot labels for the four classes, as we have already seen in the\n",
    "definition of the grounding for this task.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LTN 设置\n",
    "\n",
    "为了定义我们的知识库（公理），我们需要定义谓词 $P$、常量 $l_{blue}$、$l_{orange}$、$l_{male}$、$l_{female}$、连接词、全称量词以及 `SatAgg` 操作符。\n",
    "\n",
    "对于连接词和量词，我们使用稳定乘积配置（在教程中已经见过）。\n",
    "\n",
    "对于谓词 $P$，我们有两种模型。第一种模型实现了一个 MLP（多层感知器），它输出数据集中四个类别的logits，输入为一个示例 $x$。第二种模型输入一个标记示例 $(x, l)$，它使用第一种模型来计算logits，然后返回类别 $l$ 的预测值（*sigmoid*）。\n",
    "\n",
    "常量 $l_{blue}$、$l_{orange}$、$l_{male}$ 和 $l_{female}$ 表示四个类别的独热标签（one-hot labels），正如我们在该任务的基础定义中所见。\n",
    "\n",
    "`SatAgg` 使用 `pMeanError` 聚合器定义。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import ltn\n",
    "\n",
    "# we define the constants # 定义常量\n",
    "l_blue = ltn.Constant(torch.tensor([1, 0, 0, 0]))\n",
    "l_orange = ltn.Constant(torch.tensor([0, 1, 0, 0]))\n",
    "l_male = ltn.Constant(torch.tensor([0, 0, 1, 0]))\n",
    "l_female = ltn.Constant(torch.tensor([0, 0, 0, 1]))\n",
    "\n",
    "# we define predicate P # 定义谓词P\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model returns the logits for the classes given an input example. It does not compute the softmax, so the output\n",
    "    are not normalized.\n",
    "    This is done to separate the accuracy computation from the satisfaction level computation. Go through the example\n",
    "    to understand it.\n",
    "    该模型在给定输入示例的情况下返回类别的对数值（logits）。它不计算 softmax，因此输出未归一化。\n",
    "    这样做是为了将准确性计算与满意度水平计算分开。请通读示例以理解这一点。\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes=(5, 16, 16, 8, 4)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i]) for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Method which defines the forward phase of the neural network for our multi class classification task.\n",
    "        In particular, it returns the logits for the classes given an input example.\n",
    "\n",
    "        :param x: the features of the example\n",
    "        :param training: whether the network is in training mode (dropout applied) or validation mode (dropout not applied)\n",
    "        :return: logits for example x\n",
    "        定义神经网络前向传播阶段的方法，用于我们的多类别分类任务。\n",
    "        特别地，它返回给定输入示例的各类别的对数值（logits）。\n",
    "        \n",
    "        :param x: 示例的特征\n",
    "        :param training: 指示网络是否处于训练模式（应用 dropout）或验证模式（不应用 dropout）\n",
    "        :return: 示例 x 的对数值\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax（这里应该是笔误，应该是*sigmoid*） function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    这个模型内部包含一个对数值（logits）模型，即一个给定输入示例 $x$ 计算类别对数值的模型。这个模型的理念是将对数值和概率分开。对数值模型返回一个示例的对数值，而这个模型则在给定对数值模型的情况下返回概率。\n",
    "    \n",
    "    具体来说，它的输入是一个示例 $x$ 和一个类别标签 $l$。它将对数值模型应用于 $x$ 以获得对数值。然后，它应用 softmax（这里应该是笔误，应该是*sigmoid*） 函数以获得各类别的概率。最后，它只返回与给定类别 $l$ 相关的概率。\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.sigmoid(logits) # 将 logits 通过 Sigmoid 激活函数转换为概率。\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n",
    "\n",
    "# mlp = MLP() # 原来的写法会报错\n",
    "# P = ltn.Predicate(LogitsToPredicate(mlp)) # 原来的写法会报错\n",
    "mlp = MLP().to(ltn.device)\n",
    "P = ltn.Predicate(LogitsToPredicate(mlp).to(ltn.device))\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg # 定义连接词、量词和SatAgg\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:25:10.890932Z",
     "start_time": "2024-08-07T10:25:10.875232Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data.\n",
    "\n",
    "Then, we define functions to evaluate the model performances. The model is evaluated on the test set using the following metrics:\n",
    "- the satisfaction level of the knowledge base: measure the ability of LTN to satisfy the knowledge;\n",
    "- the classification accuracy: this time, the accuracy is defined as $1 - HL$, where $HL$ is the average Hamming loss,\n",
    "i.e. the fraction of labels predicted incorrectly, with a classification threshold of 0.5 (given an example $u$,\n",
    "if the model outputs a value greater than 0.5 for class $C$ then $u$ is deemed as belonging to class $C$)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 工具类和函数\n",
    "\n",
    "现在，我们需要定义一些工具类和函数。\n",
    "\n",
    "我们定义了一个标准的 PyTorch 数据加载器，它接受数据集作为输入，并返回一个数据批次生成器。\n",
    "特别是，我们需要为训练数据和测试数据分别定义一个数据加载器实例。\n",
    "\n",
    "然后，我们定义一些函数来评估模型的性能。模型使用以下指标在测试集上进行评估：\n",
    "- 知识库的满足度：衡量 LTN 满足知识的能力；\n",
    "- 分类准确率：这次，准确率定义为 $1 - HL$，其中 $HL$ 是平均汉明损失，即预测错误标签的比例，分类阈值为 0.5（给定一个示例 $u$，如果模型对类 $C$ 输出的值大于 0.5，那么 $u$ 被视为属于类 $C$）。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels_sex = labels[0]\n",
    "        self.labels_color = labels[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels_sex = self.labels_sex[idxlist[start_idx:end_idx]]\n",
    "            labels_color = self.labels_color[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield data, labels_sex, labels_color\n",
    "\n",
    "\n",
    "# define metrics for evaluation of the model # 定义评估模型的指标\n",
    "\n",
    "# it computes the overall satisfaction level on the knowledge base using the given data loader (train or test) # 使用给定的数据加载器（训练或测试）计算知识库的整体满意度\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels_sex, labels_color in loader:\n",
    "        x = ltn.Variable(\"x\", data)\n",
    "        x_blue = ltn.Variable(\"x_blue\", data[labels_color == 0])\n",
    "        x_orange = ltn.Variable(\"x_orange\", data[labels_color == 1])\n",
    "        x_male = ltn.Variable(\"x_male\", data[labels_sex == 2])\n",
    "        x_female = ltn.Variable(\"x_female\", data[labels_sex == 3])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_blue, P(x_blue, l_blue)),\n",
    "            Forall(x_orange, P(x_orange, l_orange)),\n",
    "            Forall(x_male, P(x_male, l_male)),\n",
    "            Forall(x_female, P(x_female, l_female)),\n",
    "            Forall(x, Not(And(P(x, l_blue), P(x, l_orange)))),\n",
    "            Forall(x, Not(And(P(x, l_male), P(x, l_female))))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "# it computes the overall accuracy of the predictions of the trained model using the given data loader # 使用给定的数据加载器计算训练模型的预测的整体准确性\n",
    "# (train or test) # （训练或测试）\n",
    "def compute_accuracy(loader, threshold=0.5): # threshold的含义是分类阈值，即大于0.5则判定为属于该类别\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels_sex, labels_color in loader:\n",
    "        # predictions = mlp(data).detach().numpy() # 原来的写法会报错\n",
    "        predictions = mlp(data.to(ltn.device)).detach().cpu().numpy()\n",
    "        labels_male = (labels_sex == 2) # 将标签转换为布尔值\n",
    "        labels_female = (labels_sex == 3)\n",
    "        labels_blue = (labels_color == 0)\n",
    "        labels_orange = (labels_color == 1)\n",
    "        onehot = np.stack([labels_blue, labels_orange, labels_male, labels_female], axis=-1).astype(np.int32) # axis=-1：在最后一个轴上堆叠。最终实现了将布尔标签转换为独热编码格式。 # astype(np.int32)：转换为 32 位整数。\n",
    "        # 将预测值二值化，并转换为整数。\n",
    "        predictions = predictions > threshold # 比较运算符，返回布尔值。\n",
    "        predictions = predictions.astype(np.int32)\n",
    "        nonzero = np.count_nonzero(onehot - predictions, axis=-1).astype(np.float32) # np.count_nonzero：计算数组中非零元素的数量。axis=-1：沿最后一个轴计算。astype(np.float32)：转换为 32 位浮点数。 # 这里计算非0的个数，因为差可能是1，也可能是-1\n",
    "        multilabel_hamming_loss = nonzero / predictions.shape[-1] # predictions.shape[-1]：预测数组的最后一个维度的大小。\n",
    "        mean_accuracy += np.mean(1 - multilabel_hamming_loss)\n",
    "        # np.mean(1 - multilabel_hamming_loss)：这里计算是先把每一行（或者说每一个螃蟹个体）的汉明损失计算出来，然后对所有螃蟹求平均值。其实和之间用所有的非0个数加起来，一起计算汉明损失是一样的。具体在ex3.md中，## 1有解释。\n",
    "\n",
    "    return mean_accuracy / len(loader)\n",
    "\n",
    "# create train and test loader # 创建训练和测试加载器\n",
    "train_loader = DataLoader(train_data, (train_sex_labels, train_color_labels), 64, shuffle=True) # 传递给DataLoader的第二个参数是元组，元组的第一个元素是性别标签，第二个元素是颜色标签\n",
    "test_loader = DataLoader(test_data, (test_sex_labels, test_color_labels), 64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:26:03.784427Z",
     "start_time": "2024-08-07T10:26:03.772105Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "### Querying\n",
    "\n",
    "To illustrate the learning of constraints by LTN, we have queried three formulas\n",
    "that were not explicitly part of the knowledge base, over time during learning:\n",
    "- $\\phi_1: \\forall x (P(x, l_{blue}) \\implies \\lnot P(x, l_{orange}))$;\n",
    "- $\\phi_2: \\forall x (P(x, l_{blue}) \\implies P(x, l_{orange}))$;\n",
    "- $\\phi_3: \\forall x (P(x, l_{blue}) \\implies P(x, l_{male}))$.\n",
    "\n",
    "For querying, we use $p=5$ when approximating the universal quantifiers with\n",
    "`pMeanError`. A higher $p$ denotes a stricter universal quantification with a stronger\n",
    "focus on outliers. We should expect $\\phi_1$ to hold true (every\n",
    "blue crab cannot be orange and vice-versa), and we should expect $\\phi_2$ (every blue crab is also orange) and $\\phi_3$\n",
    "(every blue crab is male) to be false.\n",
    "\n",
    "In the following, we define some functions computing the three formulas and the implication connective, since we need it\n",
    "to define the formulas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 学习\n",
    "\n",
    "我们定义 $D$ 为所有示例的数据集。目标函数由 $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$ 给出。\n",
    "\n",
    "在实际操作中，优化器使用以下损失函数：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)\n",
    "$$\n",
    "\n",
    "其中 $B$ 是从 $D$ 中采样的一个小批量。\n",
    "\n",
    "### 查询\n",
    "\n",
    "为了说明 LTN 学习约束的过程，我们在学习过程中查询了三个不显式包含在知识库中的公式：\n",
    "- $\\phi_1: \\forall x (P(x, l_{blue}) \\implies \\lnot P(x, l_{orange}))$;\n",
    "- $\\phi_2: \\forall x (P(x, l_{blue}) \\implies P(x, l_{orange}))$;\n",
    "- $\\phi_3: \\forall x (P(x, l_{blue}) \\implies P(x, l_{male}))$。\n",
    "\n",
    "在查询时，我们使用 $p=5$ 来通过 `pMeanError` 近似全称量词。较高的 $p$ 表示更严格的全称量化，更多地关注离群值。我们期望 $\\phi_1$ 为真（每只蓝蟹不能是橙色的，反之亦然），并且我们期望 $\\phi_2$（每只蓝蟹也是橙色的）和 $\\phi_3$（每只蓝蟹是雄性的）为假。\n",
    "\n",
    "在下文中，我们定义了一些计算三个公式和蕴涵连接词的函数，因为我们需要这些函数来定义公式。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "\n",
    "def phi1(features):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    return Forall(x, Implies(P(x, l_blue), Not(P(x, l_orange))), p=5)\n",
    "\n",
    "def phi2(features):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    return Forall(x, Implies(P(x, l_blue), P(x, l_orange)), p=5)\n",
    "\n",
    "def phi3(features):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    return Forall(x, Implies(P(x, l_blue), P(x, l_male)), p=5)\n",
    "\n",
    "# it computes the satisfaction level of a formula phi using the given data loader (train or test) # 使用给定的数据加载器（训练或测试）计算公式phi的满意度\n",
    "def compute_sat_level_phi(loader, phi):\n",
    "    mean_sat = 0\n",
    "    for features, _, _ in loader:\n",
    "        mean_sat += phi(features).value\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat # 计算得到对于这个dataloader中的数据，对于指定的公式phi，每个batch的平均的满意度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:26:08.002120Z",
     "start_time": "2024-08-07T10:26:07.996442Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we learn our LTN in the multi-class multi-label classification task using the satisfaction of the knowledge base as\n",
    "an objective. In other words, we want to learn the parameters $\\theta$ of binary predicate $P$ in such a way the three\n",
    "axioms in the knowledge base are maximally satisfied. We train our model for 500 epochs and use the `Adam` optimizer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "下面，我们在多类别多标签分类任务中使用知识库的满足度作为目标来学习我们的逻辑张量网络（LTN）。换句话说，我们希望以使知识库中的三个公理最大程度上得到满足的方式来学习二元谓词 $P$ 的参数 $\\theta$。我们训练模型500个epoch，并使用 `Adam` 优化器。"
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels_sex, labels_color) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data # 我们使用当前批次数据对变量进行实例化\n",
    "        x = ltn.Variable(\"x\", data)\n",
    "        x_blue = ltn.Variable(\"x_blue\", data[labels_color == 0])\n",
    "        x_orange = ltn.Variable(\"x_orange\", data[labels_color == 1])\n",
    "        x_male = ltn.Variable(\"x_male\", data[labels_sex == 2])\n",
    "        x_female = ltn.Variable(\"x_female\", data[labels_sex == 3])\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_blue, P(x_blue, l_blue)),\n",
    "            Forall(x_orange, P(x_orange, l_orange)),\n",
    "            Forall(x_male, P(x_male, l_male)),\n",
    "            Forall(x_female, P(x_female, l_female)),\n",
    "            Forall(x, Not(And(P(x, l_blue), P(x, l_orange)))),\n",
    "            Forall(x, Not(And(P(x, l_male), P(x, l_female))))\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training # 每20个epoch打印指标\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f | \"\n",
    "                        \"Test Sat Phi 1 %.3f | Test Sat Phi 2 %.3f | Test Sat Phi 3 %.3f \" %\n",
    "              (epoch, train_loss, compute_sat_level(train_loader),\n",
    "                        compute_sat_level(test_loader),\n",
    "                        compute_accuracy(train_loader), compute_accuracy(test_loader),\n",
    "                        compute_sat_level_phi(test_loader, phi1), compute_sat_level_phi(test_loader, phi2),\n",
    "                        compute_sat_level_phi(test_loader, phi3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-07T10:27:00.916646Z",
     "start_time": "2024-08-07T10:26:22.438015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.3986 | Train Sat 0.605 | Test Sat 0.606 | Train Acc 0.478 | Test Acc 0.438 | Test Sat Phi 1 0.512 | Test Sat Phi 2 0.708 | Test Sat Phi 3 0.685 \n",
      " epoch 20 | loss 0.3702 | Train Sat 0.630 | Test Sat 0.631 | Train Acc 0.531 | Test Acc 0.506 | Test Sat Phi 1 0.541 | Test Sat Phi 2 0.744 | Test Sat Phi 3 0.753 \n",
      " epoch 40 | loss 0.3491 | Train Sat 0.652 | Test Sat 0.651 | Train Acc 0.798 | Test Acc 0.775 | Test Sat Phi 1 0.587 | Test Sat Phi 2 0.755 | Test Sat Phi 3 0.775 \n",
      " epoch 60 | loss 0.2753 | Train Sat 0.728 | Test Sat 0.718 | Train Acc 0.931 | Test Acc 0.913 | Test Sat Phi 1 0.668 | Test Sat Phi 2 0.576 | Test Sat Phi 3 0.730 \n",
      " epoch 80 | loss 0.2107 | Train Sat 0.796 | Test Sat 0.776 | Train Acc 0.948 | Test Acc 0.913 | Test Sat Phi 1 0.743 | Test Sat Phi 2 0.405 | Test Sat Phi 3 0.624 \n",
      " epoch 100 | loss 0.1376 | Train Sat 0.852 | Test Sat 0.849 | Train Acc 0.986 | Test Acc 0.975 | Test Sat Phi 1 0.840 | Test Sat Phi 2 0.252 | Test Sat Phi 3 0.469 \n",
      " epoch 120 | loss 0.1216 | Train Sat 0.880 | Test Sat 0.871 | Train Acc 0.987 | Test Acc 0.962 | Test Sat Phi 1 0.875 | Test Sat Phi 2 0.232 | Test Sat Phi 3 0.395 \n",
      " epoch 140 | loss 0.1061 | Train Sat 0.893 | Test Sat 0.882 | Train Acc 0.984 | Test Acc 0.981 | Test Sat Phi 1 0.882 | Test Sat Phi 2 0.225 | Test Sat Phi 3 0.381 \n",
      " epoch 160 | loss 0.1030 | Train Sat 0.897 | Test Sat 0.884 | Train Acc 0.990 | Test Acc 0.962 | Test Sat Phi 1 0.887 | Test Sat Phi 2 0.218 | Test Sat Phi 3 0.359 \n",
      " epoch 180 | loss 0.0972 | Train Sat 0.908 | Test Sat 0.894 | Train Acc 0.987 | Test Acc 0.988 | Test Sat Phi 1 0.905 | Test Sat Phi 2 0.206 | Test Sat Phi 3 0.349 \n",
      " epoch 200 | loss 0.1064 | Train Sat 0.901 | Test Sat 0.906 | Train Acc 0.987 | Test Acc 0.988 | Test Sat Phi 1 0.907 | Test Sat Phi 2 0.204 | Test Sat Phi 3 0.358 \n",
      " epoch 220 | loss 0.0907 | Train Sat 0.909 | Test Sat 0.898 | Train Acc 0.986 | Test Acc 0.988 | Test Sat Phi 1 0.903 | Test Sat Phi 2 0.203 | Test Sat Phi 3 0.340 \n",
      " epoch 240 | loss 0.0865 | Train Sat 0.912 | Test Sat 0.906 | Train Acc 0.984 | Test Acc 0.988 | Test Sat Phi 1 0.913 | Test Sat Phi 2 0.198 | Test Sat Phi 3 0.333 \n",
      " epoch 260 | loss 0.0914 | Train Sat 0.916 | Test Sat 0.904 | Train Acc 0.986 | Test Acc 0.988 | Test Sat Phi 1 0.896 | Test Sat Phi 2 0.202 | Test Sat Phi 3 0.336 \n",
      " epoch 280 | loss 0.0891 | Train Sat 0.924 | Test Sat 0.905 | Train Acc 0.983 | Test Acc 0.988 | Test Sat Phi 1 0.897 | Test Sat Phi 2 0.201 | Test Sat Phi 3 0.331 \n",
      " epoch 300 | loss 0.0812 | Train Sat 0.914 | Test Sat 0.918 | Train Acc 0.983 | Test Acc 1.000 | Test Sat Phi 1 0.914 | Test Sat Phi 2 0.194 | Test Sat Phi 3 0.339 \n",
      " epoch 320 | loss 0.0868 | Train Sat 0.926 | Test Sat 0.903 | Train Acc 0.986 | Test Acc 0.981 | Test Sat Phi 1 0.903 | Test Sat Phi 2 0.196 | Test Sat Phi 3 0.313 \n",
      " epoch 340 | loss 0.0764 | Train Sat 0.926 | Test Sat 0.905 | Train Acc 0.983 | Test Acc 0.975 | Test Sat Phi 1 0.921 | Test Sat Phi 2 0.192 | Test Sat Phi 3 0.313 \n",
      " epoch 360 | loss 0.0672 | Train Sat 0.921 | Test Sat 0.882 | Train Acc 0.986 | Test Acc 0.962 | Test Sat Phi 1 0.932 | Test Sat Phi 2 0.189 | Test Sat Phi 3 0.290 \n",
      " epoch 380 | loss 0.0810 | Train Sat 0.926 | Test Sat 0.911 | Train Acc 0.983 | Test Acc 0.981 | Test Sat Phi 1 0.946 | Test Sat Phi 2 0.184 | Test Sat Phi 3 0.305 \n",
      " epoch 400 | loss 0.0760 | Train Sat 0.928 | Test Sat 0.914 | Train Acc 0.984 | Test Acc 0.988 | Test Sat Phi 1 0.926 | Test Sat Phi 2 0.188 | Test Sat Phi 3 0.309 \n",
      " epoch 420 | loss 0.0676 | Train Sat 0.924 | Test Sat 0.897 | Train Acc 0.992 | Test Acc 0.962 | Test Sat Phi 1 0.895 | Test Sat Phi 2 0.195 | Test Sat Phi 3 0.299 \n",
      " epoch 440 | loss 0.0721 | Train Sat 0.930 | Test Sat 0.900 | Train Acc 0.990 | Test Acc 0.975 | Test Sat Phi 1 0.939 | Test Sat Phi 2 0.184 | Test Sat Phi 3 0.291 \n",
      " epoch 460 | loss 0.0670 | Train Sat 0.928 | Test Sat 0.925 | Train Acc 0.993 | Test Acc 1.000 | Test Sat Phi 1 0.921 | Test Sat Phi 2 0.187 | Test Sat Phi 3 0.314 \n",
      " epoch 480 | loss 0.0625 | Train Sat 0.927 | Test Sat 0.925 | Train Acc 0.992 | Test Acc 1.000 | Test Sat Phi 1 0.924 | Test Sat Phi 2 0.187 | Test Sat Phi 3 0.309 \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that variables $x_{blue}$, $x_{orange}$, $x_{male}$, and $x_{female}$ are grounded batch by batch with new data\n",
    "arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg` takes as input the four axioms and returns one truth value which can be interpreted as the satisfaction\n",
    "level of the knowledge base.\n",
    "\n",
    "Note that after 100 epochs the test accuracy is around 1. This shows the power of LTN in learning\n",
    "the multi-class multi-label classification task only using the satisfaction of a knowledge base as an objective.\n",
    "\n",
    "At the beginning of the training, the truth values of $\\phi_1$, $\\phi_2$, and $\\phi_3$ are non-informative. Instead, during\n",
    "training, one can see a trend towards the satisfaction of $\\phi_1$, and an opposite trend for $\\phi_2$ and $\\phi_3$, as expected.\n",
    "This shows the ability of LTN to query and reason on never-seen formulas.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "当然，以下是您提供的内容的中文翻译：\n",
    "\n",
    "请注意，变量 $ x_{blue} $, $ x_{orange} $, $ x_{male} $, 和 $ x_{female} $ 是通过数据加载器逐批接收新数据进行基础化的。这正是我们所指的 $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$，其中 $B$ 是由数据加载器采样的小批量数据。\n",
    "\n",
    "还请注意，`SatAgg` 接受四个公理作为输入并返回一个真值，该真值可以解释为知识库的满意度水平。\n",
    "\n",
    "请注意，在经过 100 轮训练后，测试准确率约为 1。这显示了 LTN 在仅使用知识库满意度作为目标来学习多类别多标签分类任务中的强大能力。\n",
    "\n",
    "在训练开始时，$\\phi_1$, $\\phi_2$, 和 $\\phi_3$ 的真值是无信息的。相反，在训练过程中，可以看到 $\\phi_1$ 的满意度呈上升趋势，而 $\\phi_2$ 和 $\\phi_3$ 的满意度则呈下降趋势，正如预期的那样。这显示了 LTN 查询和推理从未见过的公式的能力。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们在训练的时候，使用了一些公式，我们后来随着训练的不断推进，在这个过程中，不断去查询（query）一些新的公式（phi 1，2，3），这些公式模型是没有见过的，但是我们训练使得模型学习到了知识库中已有的公式，根据这些已有的公式大概是可以推导出这些新的公式应该是真还是假的，所以我们可以看到，随着训练的不断推进，这些新的公式的满意度也在不断变化，该变大的在变大（即该公式在知识库的情况下，应该是真的），该变小的在变小（即该公式在知识库的情况下，应该是假的）。这说明了LTN的强大能力。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
